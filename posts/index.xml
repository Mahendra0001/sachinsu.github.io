<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Sachin Sunkle</title><link>https://sachinsu.github.io/posts/</link><description>Recent content in Posts on Sachin Sunkle</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Thu, 25 Feb 2021 00:00:00 +0530</lastBuildDate><atom:link href="https://sachinsu.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Upgrading API: Learnings</title><link>https://sachinsu.github.io/posts/apiupgrade/</link><pubDate>Thu, 25 Feb 2021 00:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/apiupgrade/</guid><description>Introduction One of the design considerations stressed upon by Jeffrey richter about APIs (Read more here) is that &amp;ldquo;API is expected to be stable over long period of time&amp;rdquo;. Recently,for a .NET based project, we decided to upgrade some of the ASMX (legacy SOAP based approach) based APIs and were immediately reminded by Customer(s) to avoid any kind of impact on existing users.
This means that upgrade must be done keeping in mind,</description><content type="html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>One of the design considerations stressed upon by Jeffrey richter about APIs (Read more <a href="/posts/restapiversioning/">here</a>) is that &ldquo;API is expected to be stable over long period of time&rdquo;. Recently,for a .NET based project, we decided to upgrade some of the ASMX (legacy SOAP based approach) based APIs and were immediately reminded by Customer(s) to avoid any kind of impact on existing users.</p>
<p>This means that upgrade must be done keeping in mind,</p>
<ul>
<li>No changes to API Contract (SOAP remains SOAP and so on)</li>
<li>No changes to URLs</li>
<li>Testing to ensure no impact</li>
</ul>
<p>Initial plan was to move away from SOAP to adopt REST based approach. This thinking was aided by fact that .NET core may not support WCF (framework that supports SOAP apart from ASMX Web Services) in addition to other aspects like simplicity and wide adoption of REST. However, even microsoft has now decided to support WCF in .NET Core via <a href="https://github.com/CoreWCF/CoreWCF">CoreWCF</a>.</p>
<p>With this constraints, below alternatives were considered to upgrade ASMX based services to WCF (the only other framework that supports SOAP based services),</p>
<table>
<thead>
<tr>
<th style="text-align:left">Approach</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">Have existing ASMX Service call new WCF Service using Async/Await</td>
<td style="text-align:left">This involves hosting additional WCF Service and making HTTP requests to it. It also means maintaining both ASMX &amp; WCF endpoints. Also to be mindful of latency introduced due to HTTP communication between the two.</td>
</tr>
<tr>
<td style="text-align:left">New WCF Service and URL Rewrite rules to handle requests to ASMX</td>
<td style="text-align:left">This involves developing new WCF Service, compatible to current service contract, and configuration to route/re-write incoming requests to new service. Existing ASMX end point can be sunset</td>
</tr>
<tr>
<td style="text-align:left">New WCF Service and mapping <code>.asmx</code> handler to WCF handler</td>
<td style="text-align:left">This involves developing new WCF Service,compatible to current service contract, and configuration so that requests to <code>.asmx</code> url will be served by WCF handler. Existing ASMX end point can be sunset.</td>
</tr>
</tbody>
</table>
<p>Lets go through above approaches in detail.</p>
<h2 id="wcf-service-invoked-from-asmx-asynchronously">WCF service invoked from ASMX asynchronously</h2>
<p>This involves developing new WCF Service. Existing ASMX based web service will be modified to invoke new WCF Service. Asynchronously invocation should help in this case since whole operation is I/O bound (<code>Asynchrony is a way to get concurrency without multithreading. E.g., freeing up the calling thread instead of blocking it while an I/O operation is in progress</code> Stephen Cleary). Since ASMX is legacy framework and only support Event-based asynchronous pattern (EAP), it is necessary to combine EAP with Task based asynchronous pattern (TAP) which is what async/await uses. Below is sample snippet,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback"> private async Task&lt;string&gt; FooAsync(int arg)
        {
            using (var resp = await client.GetAsync(string.Format(&#34;https://jsonplaceholder.typicode.com/todos/{0}&#34;, arg)).ConfigureAwait(false)) {
                resp.EnsureSuccessStatusCode();

                using (var contentStream = await resp.Content.ReadAsStreamAsync().ConfigureAwait(false)) { 

                    APIResponse obj = await JsonSerializer.DeserializeAsync&lt;APIResponse&gt;(contentStream).ConfigureAwait(false);

                   string output =  string.Format(&#34;{0} at {1}&#34;, obj.Title, DateTime.Now.Ticks);
                    System.Diagnostics.Debug.WriteLine(output);
                    return output;
                }
            }

        }

        [WebMethod]
        public IAsyncResult BeginFoo(int arg, AsyncCallback callback, object state)
        {
            return FooAsync(arg).ToApm(callback, state);           
        }

        [WebMethod]
        public string EndFoo(IAsyncResult result)
        {
            try
            {
                return ((Task&lt;string&gt;)result).Result;
            }
            catch (AggregateException ae) { throw ae.InnerException; }
        }

</code></pre></div><p>Where <code>ToApm</code> is extension function from Stephen Toub&rsquo;s excellent blog (link in code as comment),</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">
        public static Task&lt;TResult&gt; ToApm&lt;TResult&gt;(this Task&lt;TResult&gt; task, AsyncCallback callback, object state)
        {
            if (task.AsyncState == state)
            {
                if (callback != null)
                {
                    task.ContinueWith(delegate { callback(task); },
                        CancellationToken.None, TaskContinuationOptions.None, TaskScheduler.Default);
                }
                return task;
            }

            var tcs = new TaskCompletionSource&lt;TResult&gt;(state);
            task.ContinueWith(delegate
            {
                if (task.IsFaulted) tcs.TrySetException(task.Exception.InnerExceptions);
                else if (task.IsCanceled) tcs.TrySetCanceled();
                else tcs.TrySetResult(task.Result);

                if (callback != null) callback(tcs.Task);

            }, CancellationToken.None, TaskContinuationOptions.None, TaskScheduler.Default);
            return tcs.Task;
        }

</code></pre></div><p>This approach involves,</p>
<ul>
<li>hosting and maintaining both (current and new) API end-points.</li>
<li>We also came across issues where async/await was not working properly in case code blocks.</li>
<li>Measuring and mitigating any latency induced due to this additional hop</li>
<li>Additional Monitoring and logging to track WCF end-point</li>
</ul>
<p>We decided to explore alternative approaches instead of this.</p>
<h2 id="wcf-service-with-url-re-write">WCF service with URL re-write</h2>
<p>This requires hosting WCF Service which is backward compatible with ASMX based SOAP implementation.</p>
<p>Typically this involves,</p>
<ul>
<li>supporting <code>basicHttpBinding</code></li>
<li>Adding namespaces and support for XML Serialization to Service contract like,</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">
  [ServiceContract(Name = &#34;RequestReplyService&#34;, Namespace = &#34;http://tempuri.org/&#34;),XmlSerializerFormat]

</code></pre></div><ul>
<li>Adding Action to OperationContract attribute like,</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">[OperationContract(IsOneWay = false, Action = &#34;http://tempuri.org/DoWork&#34;)]

</code></pre></div><p>Additional configuration to re-write incoming requests to <code>.asmx</code> to new service in <code>web.config</code> as below,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">
 &lt;system.webServer&gt;
    &lt;validation validateIntegratedModeConfiguration=&#34;false&#34; /&gt;    
    &lt;rewrite&gt;
      &lt;rules&gt;
        &lt;rule name=&#34;asmxtosvc&#34; stopProcessing=&#34;true&#34;&gt;
          &lt;match url=&#34;^service.asmx(.*)$&#34; /&gt;
          &lt;action type=&#34;Rewrite&#34; url=&#34;Service.svc{R:1}&#34;/&gt;
        &lt;/rule&gt;
      &lt;/rules&gt;
    &lt;/rewrite&gt;
  &lt;/system.webServer&gt;

</code></pre></div><p>One may want to test above re-write settings in IIS as older versions of it require installation of URL Rewrite module.</p>
<p>This is followed by testing new WCF service from existing client(s), be it .NET based clients or other ones with no changes. .NET based clients typically invoke service through generated proxy class. For other clients, we basically simulated it via Postman.</p>
<p>This approach provides cleaner implementation vis-a-vis earlier approach such that it is still new WCF based implementation with no ASMX in use.</p>
<h2 id="wcf-service-with-asmx-extension-mapped-to-wcf-handler">WCF service with .asmx extension mapped to WCF handler</h2>
<p>This approach is very similar to last one with only change being instead of using URL re-write module, we will map <code>.asmx</code> extension to WCF Handler. So the changes are only in web.config as below,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">
&lt;system.web&gt;
    &lt;httpHandlers&gt;
    &lt;remove path=&#34;.asmx&#34; verb=&#34;*&#34; /&gt;
    &lt;add path=&#34;*.asmx&#34; verb=&#34;*&#34; type=&#34;System.ServiceModel.Activation.HttpHandler, System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&#34; validate=&#34;false&#34; /&gt;
    &lt;/httpHandlers&gt;
    &lt;compilation debug=&#34;true&#34; targetFramework=&#34;4.8&#34;&gt;
    &lt;buildProviders&gt;
        &lt;remove extension=&#34;.asmx&#34;/&gt;
        &lt;add extension=&#34;.asmx&#34; type=&#34;System.ServiceModel.Activation.ServiceBuildProvider, System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089&#34;/&gt;
      &lt;/buildProviders&gt;
    &lt;/compilation&gt;
    &lt;httpRuntime targetFramework=&#34;4.8&#34;/&gt;
  &lt;/system.web&gt;
....


&lt;system.webServer&gt;
  &lt;handlers&gt;
    &lt;remove name=&#34;WebServiceHandlerFactory-Integrated&#34;/&gt;
    &lt;add name=&#34;MyNewAsmxHandler&#34; path=&#34;*.asmx&#34; verb=&#34;*&#34; type=&#34;System.ServiceModel.Activation.HttpHandler, System.ServiceModel.Activation, Version=4.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35&#34; /&gt;
  &lt;/handlers&gt;
  &lt;validation validateIntegratedModeConfiguration=&#34;false&#34; /&gt;
&lt;/system.webServer&gt;

</code></pre></div><p>This was tested in same way as earlier with existing .NET and other clients.</p>
<p>This feels like even more cleaner approach than using URL re-write as it doesn&rsquo;t involve using any additional module/library.</p>
<p>Finally, we went ahead with this approach.</p>
<p>Hopefully,this article will be helpful to anyone involved in legacy modernization initiatives.</p>
<h3 id="useful-references">Useful References</h3>
<ul>
<li><a href="https://docs.microsoft.com/en-us/dotnet/framework/wcf/feature-details/comparing-aspnet-web-services-to-wcf-based-on-development">Comparing ASMX web services to WCF</a></li>
<li><a href="https://devblogs.microsoft.com/pfxteam/using-tasks-to-implement-the-apm-pattern/">APM Pattern using Tasks</a></li>
<li><a href="https://blog.stephencleary.com/2012/08/async-wcf-today-and-tomorrow.html">Async in WCF</a></li>
<li><a href="https://docs.microsoft.com/en-us/dotnet/framework/wcf/feature-details/comparing-aspnet-web-services-to-wcf-based-on-development">Comparing ASMX with WCF</a></li>
<li><a href="https://stackoverflow.com/questions/5686320/asmx-to-wcf-conversion">Discussion on ASMX to WCF</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Learnings from Jeff Richter's Designing and Versioning HTTP REST APIs Video Course</title><link>https://sachinsu.github.io/posts/restapiversioning/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/restapiversioning/</guid><description>Background Recently, i went through excellent video series on Designing &amp;amp; Versioning HTTP_REST APIs presented by Jeffrey Richter. It is available here. In the past, i had read Jeff&amp;rsquo;s books on CLR and found his writing to be very clear and understandable. So is my experience with this Video Series. Below is summary of learnings from this Video Series. I do not claim that every aspect is covered here so please do check out the videos.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>Recently, i went through excellent video series on  <code>Designing &amp; Versioning HTTP_REST APIs</code> presented by <a href="https://www.linkedin.com/in/jeffrichter/">Jeffrey Richter</a>. It is available <a href="https://www.youtube.com/watch?v=9Ng00IlBCtw">here</a>. In the past, i had read Jeff&rsquo;s books on CLR and found his writing to be very clear and understandable. So is my experience with this Video Series. Below is summary of learnings from this Video Series. I do not claim that every aspect is covered here so please do check out the videos.</p>
<p>I have been developing REST APIs for many years  but the video series opened up many new aspects that were previously unknown. Jeff starts with Need to Good API Design and related considerations, REST Fundamentals and then dives deeper into aspects like idempotent behavior, versioning, ETags and so on.</p>
<p>Jeff covers REST fundamentals, need for thoughtful API design as it might be difficult to amend it later followed by practices covering Naming,Need for Idempotency, Error Handling and so on.  Below is an attempt to summarize aspects from these videos.</p>
<h4 id="rest-fundamentals">REST Fundamentals</h4>
<p>REST is an architectural style with emphasis on,</p>
<ul>
<li>Scalability</li>
<li>Reduced latency via Caching</li>
<li>Independent Deployment</li>
<li>Encapsulating legacy Systems</li>
</ul>
<p>A REST service has resources where they represent state but not behavior. These behaviors are CRUD (Created, Read, Update and Delete). All operations of service must be idempotent.</p>
<p>URL of the REST Service is expected to be stable over long period of time. URL (apart from HTTP scheme and host name:port), consists of</p>
<ul>
<li>Document (eg. <code>song-management</code>) - sometimes omitted in which case the host determines document.</li>
<li>Collection resource (<code>users</code> or <code>playlists</code>) - Hold items; use plural lowercase noun; avoid more than 2 collections.</li>
<li>Item resource - Request/Response body holds the item&rsquo;s state</li>
<li>Method - Prefer &lsquo;PATCH&rsquo;, with JSON Merge Patch request body, over &lsquo;PUT&rsquo;. &lsquo;PUT&rsquo; for whole creation or replacement but may introduce  issues between different versions of service. Avoid &lsquo;POST&rsquo; as it involves challenges in ensuring Idempotent behavior.  The argument against &lsquo;POST&rsquo; is that it always creates resource and returns identifier which may get lost and client may retry. .</li>
</ul>
<h4 id="error-handling">Error Handling</h4>
<p>Videos contain tables explaining HTTP status code to be returned along with body in different conditions. below is quick summary,</p>
<ul>
<li>
<p>If something unexpected happens return Status 500</p>
</li>
<li>
<p>If service is booting, too busy or shutting down then return Status 303</p>
</li>
<li>
<p>If HTTP Version not 1.1 then return status 505</p>
</li>
<li>
<p>If Authorization fails then return status 401</p>
</li>
<li>
<p>If Client makes too many requests/second then return status 429</p>
</li>
<li>
<p>If URL too long then return status 414</p>
</li>
<li>
<p>If HTTP Method not supported at all then return status 501</p>
</li>
<li>
<p>If resource not accessible to client then return status 403</p>
</li>
<li>
<p>If No support for HTTP Method not 1.1 then return status 405</p>
</li>
<li>
<p>If request not in acceptable format then return status 406</p>
</li>
<li>
<p>In case service returns non-success/error response for a request,</p>
<ul>
<li>
<p>It is recommended to add header in response that indicates the same (that way client can inspect it before deserializing/inspecting the response).</p>
</li>
<li>
<p>Also if error is recoverable @ runtime then, string is specific else string is list of similar errors. Response body could be in JSON format as,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-JSON" data-lang="JSON">
{
	<span style="color:#f92672">&#34;error&#34;</span>: { 
        <span style="color:#f92672">&#34;code&#34;</span>:<span style="color:#e6db74">&#34;&#34;</span>,
        <span style="color:#f92672">&#34;message&#34;</span>:<span style="color:#e6db74">&#34;&#34;</span>,
        <span style="color:#f92672">&#34;target&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>,
        <span style="color:#f92672">&#34;innererror&#34;</span>: {
            <span style="color:#f92672">&#34;code&#34;</span>: <span style="color:#e6db74">&#34;&#34;</span>, 
            <span style="color:#f92672">&#34;minLength&#34;</span>: <span style="color:#ae81ff">6</span>
        }
	}
}

</code></pre></div></li>
</ul>
</li>
<li>
<p>If server is overloaded then return 503</p>
</li>
<li>
<p>If tenant exceeds quota then return 429</p>
</li>
</ul>
<h3 id="versioning">Versioning</h3>
<ul>
<li>
<p>New version must be backward Compatible</p>
</li>
<li>
<p>Examples of versioning in API,</p>
<ul>
<li><code>http://api.contoso.com/v1.0/products</code></li>
<li><code>http://api.contoso.com/products?api-version=1.0</code></li>
<li><code>http://api.contoso.com/products?api-version=2021-01-01</code></li>
</ul>
</li>
<li>
<p>Add new API when changing mandatory parameters, payload formats, error codes  or behavior</p>
</li>
<li>
<p>Approach to API Versioning should not be afterthought</p>
</li>
<li>
<p>Consider embedding version number in the data structure</p>
</li>
</ul>
<h3 id="checklist-for-rest-apis">Checklist for REST APIs</h3>
<ul>
<li>
<p>Focus on great and consistent naming - This is very important because once in production, this is unlikely to change.</p>
</li>
<li>
<p>Ensure that resource path make sense</p>
</li>
<li>
<p>Always try to simplify call to Service (by having fewer query parameters &amp; JSON fields)</p>
</li>
<li>
<p>Use</p>
<ul>
<li>
<p><code>PUT</code> to Create/Replace whole resources. Last write wins. It should return <code>200-Ok</code>, <code>201-Created</code>.</p>
</li>
<li>
<p><code>PATCH</code> to Create/Modify resource with JSON Merge Patch. It should return <code>200-Ok</code>, <code>201-Created</code>.</p>
</li>
<li>
<p><code>GET</code> to Get the resource.It should return <code>200-Ok</code>.</p>
</li>
<li>
<p><code>DELETE</code> to remove resource. It should return <code>200-Ok</code>, <code>204-No content</code> but  <code>404-not found</code> should be avoided.</p>
<p>Jeff recommends avoiding usage of <code>POST</code> unless request is idempotent (HTTP does not require it to be idempotent).For API having <code>POST</code> operation, below Idempotency Pattern can be considered,</p>
<ul>
<li>Client: Creates ID</li>
<li>Client: sends request to server with generated ID (this can be retried)</li>
<li>Server: If ID is not in log then, do operation &amp; log ID (This should be part of transaction); respond with OK (Server periodically deletes old log to avoid unbounded growth)</li>
</ul>
</li>
</ul>
</li>
<li>
<p>A URL should be stable/immutable</p>
</li>
<li>
<p>Use proper response codes to enable customers to self-fix</p>
</li>
<li>
<p>Have clear contracts for string values</p>
</li>
<li>
<p>Share samples (Code) that really use your API</p>
</li>
<li>
<p>Use Etag (Entity Tag) to identify the version of the resource,</p>
<ul>
<li>Etag is usually computed as checksum or as sequence value (which is incremented on every change)</li>
<li>for single item response, it is set in header and for collections, it is added as field in body.</li>
<li>Caching - Clients can use it for resource caching (send GET Request with ETag and server responds with 304-Not modified if resource hasn&rsquo;t changed)</li>
<li>Concurrent/Conditional Updates -Etag along with &lsquo;if-none-match&rsquo;/&lsquo;if-match&rsquo; headers allows conditional update/delete</li>
</ul>
</li>
<li>
<p>Services must fail fast if requests are greater than quota (requests/time)</p>
</li>
<li>
<p>Every request must be assigned unique ID for logging /tracing purposes. this ID can be returned in header of response.</p>
</li>
<li>
<p>Generate Telemetry for the Service. It should include User Agent information for diagnostic purposes. Also consider adding Distributed tracing (OpenTelemetry is standardization initiative in this regard).</p>
</li>
<li>
<p>In case of client retries, services must be idempotent (Idempotency indicates retrying a request has same intended effect, even if the original request succeeded, though response might differ)</p>
</li>
<li>
<p>Service must remain fault-tolerant in case of failures.</p>
</li>
<li>
<p>Typically REST is meant for State transfer/CRUD Operations but many times the purpose of end point is to offer action. In such cases specify the action being performed at the end , i.e. after establishing the exact resource, of URL like, <code>/user-management/users/{userid}/:send-sms</code>. In this,</p>
<ul>
<li>&lsquo;user-management&rsquo; indicates host</li>
<li>&lsquo;users&rsquo; indicates users collection</li>
<li>&lsquo;{userid}&rsquo; is to identify user by ID</li>
<li>&lsquo;:send-sms&rsquo; indicates action (prefixed with &lsquo;:') to be performed.</li>
</ul>
</li>
<li>
<p>Use tools like <a href="https://swagger.io">Swagger</a> for API documentation and to create language-specific client libraries</p>
</li>
</ul>
<h4 id="reviewing-rest-apis">Reviewing REST APIs</h4>
<p>While reviewing HTTP REST APis, below aspects should be evaluated,</p>
<ul>
<li>
<p>Does the API Match Customer&rsquo;s Expectation? Aspects to check are,</p>
<ul>
<li>URLs</li>
<li>idempotency</li>
<li>atomicity</li>
<li>json</li>
<li>casing</li>
<li>status codes</li>
<li>paging</li>
<li>long running operations</li>
</ul>
</li>
<li>
<p>Consistency with other Services</p>
</li>
<li>
<p>Is the Service/API sustainable over time i.e. API must be able to grow/version over time without breaking customer apps</p>
</li>
</ul>
<p>In no way, the above covers everything available in the Video series. So do check it out <a href="https://www.youtube.com/watch?v=9Ng00IlBCtw">here</a>.</p>
<h3 id="useful-references">Useful References</h3>
<ul>
<li><a href="https://d1.awsstatic.com/builderslibrary/pdfs/making-retries-safe-with-idempotent-apis-malcolm-featonby.pdf">Making retries safe with Idempotent APIs</a></li>
</ul>
<p>Happy API designing !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Resiliency Testing with Toxiproxy</title><link>https://sachinsu.github.io/posts/resiliencytoxiproxy/</link><pubDate>Sat, 09 Jan 2021 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/resiliencytoxiproxy/</guid><description>Background In a typical workflow of software development, Developer implements a Unit/component, tests it and pushes the changes to source control repository. It then goes through Continuous integration, automated testing, provisioning and deployment. Given High availability requirements expected (or should i say assumed) nowadays, As much as functional correctness of the Unit, it is also important to test how a Unit/Component handles failures, delays etc. in distributed environment. Often, such behavior is observed in production itself, unless project team is following practices of Chaos engineering.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>In a typical workflow of software development, Developer implements a Unit/component, tests it and pushes  the changes to source control repository. It then goes through Continuous integration, automated testing, provisioning and deployment. Given High availability requirements expected (or should i say assumed) nowadays,  As much as functional correctness of the Unit, it is also important to test how a Unit/Component handles failures, delays etc. in distributed environment.  Often, such behavior is observed in production itself, unless project team is following practices of <a href="https://netflixtechblog.com/tagged/chaos-engineering">Chaos engineering</a>.</p>
<p>Wouldn&rsquo;t it be great if it is possible to start testing the resiliency features as part of development and during CI/CD pipeline execution itself ? Enter <a href="https://toxiproxy.io">Toxiproxy</a></p>
<p>Toxiproxy is a TCP Proxy to simulate network and system conditions for chaos and resiliency Testing.</p>
<p>Toxiproxy essentially acts as middleman between your application and remote service/system being tested, allowing injection of delays, simulate Bandwidth restriction or turn interface off (down)
etc.</p>
<p>It provides CLI as well as http API for applications to  inject these behaviors or toxics. Refer <a href="https://github.com/shopify/toxiproxy#toxics">here</a> for various toxics supported.</p>
<h3 id="lets-use-toxiproxy">Lets use Toxiproxy</h3>
<p>Lets see how one can use it in typical use case where Application uses PostgreSQL database and requirement is to benchmark it against database hosted remotely. Toxiproxy can help simulate production like behavior by means of introducing network delay.</p>
<p>The full source code of this application is available <a href="https://github.com/sachinsu/toxiproxyPOC">here</a>. One can refer to Numbers (only as reference cause live conditions may widely vary) <a href="https://github.com/sirupsen/napkin-math">here</a> while deciding on how much toxicity to introduce.</p>
<p>Application itself is a Web server in <a href="https://golang.org">Go</a> using excellent <a href="https://github.com/julienschmidt/httprouter">HTTPRouter</a>, It does,</p>
<ul>
<li>
<p>provision a table in Postgresql and load <a href="https://github.com/sachinsu/toxiproxyPOC/tree/main/assets">dummy data</a> in it.</p>
</li>
<li>
<p>Exposes REST API that reads data from database and returns JSON</p>
</li>
<li>
<p>Setup proxy between application and database either through Toxiproxy CLI (it can also be done programmatically using Toxiproxy-Go client),</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">    [
        {
            &#34;name&#34;: &#34;pgsql&#34;, 
            &#34;listen&#34; : &#34;[::]:6000&#34;,
            &#34;upstream&#34; : &#34;127.0.0.1:5432&#34;,
            &#34;enabled&#34;: true
        }
    ]
</code></pre></div><p>or through Code like,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">    // InjectLatency helper
    func InjectLatency(name string, delay int) *toxiproxy.Proxy {
        proxy, err := toxiClient.Proxy(name)

        if err != nil {
            panic(err)
        }

        proxy.AddToxic(&#34;&#34;, &#34;latency&#34;, &#34;&#34;, 1, toxiproxy.Attributes{
            &#34;latency&#34;: delay,
        })
        return proxy
    }
</code></pre></div></li>
<li>
<p>Use <a href="https://github.com/rakyll/hey">hey</a> or any other HTTP Benchmarking tool to test end points with and without toxicity</p>
</li>
</ul>
<p>or</p>
<ul>
<li>Go <a href="https://dave.cheney.net/2013/06/30/how-to-write-benchmarks-in-go">benchmark</a> <a href="https://github.com/sachinsu/toxiproxyPOC/tree/main/api/server">tests</a> tests that are executed against HTTP end points.</li>
</ul>
<p>In my opinion, Toxiproxy allows us to embed aspect(s) of resiliency verification in the code itself so developer can test it before committing the code and it can be embedded in DevOps pipeline to get early feedback before facing the music in production environment.</p>
<p>Like latency, one can easily introduce other Toxics like Bandwidth, Down and Timeout to check Application&rsquo;s behavior when faced with such occurrences.</p>
<h3 id="useful-references">Useful References,</h3>
<ul>
<li><a href="https://toxiproxy.io">ToxiProxy</a> - for all details on the tool like Clients available in various languages, server releases and so on.</li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Using Temporal.io to build Long running Workflows</title><link>https://sachinsu.github.io/posts/temporalworkflow/</link><pubDate>Mon, 07 Dec 2020 08:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/temporalworkflow/</guid><description>Background In a typical business Application, there are often requirements for,
Batch processing - Often long running Tasks like data import/export, End of day processing etc. These tasks are often scheduled to be executed at pre-defined interval or on occurance of an Event. Asychronous processing - Tasks, often part of business process / workflow, that can be performed asychronously or offloaded. Such requirements are often fulfilled with custom approaches like batch processing frameworks, ETL Tools or using Queues or specific database features.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>In a typical business Application, there are often requirements for,</p>
<ul>
<li>Batch processing - Often long running Tasks like data import/export, End of day processing etc. These tasks are often scheduled to be executed at pre-defined interval or on occurance of an Event.</li>
<li>Asychronous processing - Tasks, often part of business process / workflow, that can be performed asychronously or offloaded.</li>
</ul>
<p>Such requirements are often fulfilled with custom approaches like batch processing frameworks, ETL Tools or using Queues or specific database features.</p>
<p>I had been following how Uber fulfils these requirements using their <a href="https://cadenceworkflow.io/">Cadence</a> platform. Cadence (now <a href="https://temporal.io">Temporal</a>)  is a distributed, scalable, durable, and highly available orchestration engine to execute asynchronous long-running business logic in a scalable and resilient way.</p>
<p>Temporal defines <a href="https://softwareengineeringdaily.com/wp-content/uploads/2020/04/SED1043-Cadence-Workflow-Orchestration.pdf">workflow</a> as any program which,</p>
<ul>
<li>goes beyond single request-reply</li>
<li>has multiple steps tied together with inherent state</li>
<li>can be short or long lived.</li>
<li>performs Event processing</li>
<li>involves infrastructure automation</li>
</ul>
<p>This is interesting perspective that accommodates various use cases irrespective of architecture style (i.e. Monolith, Microservices) in use. In Temporal, one has to create workflow which in turn consists of one or more activities. Activities are functions containing actions to be taken on each step of the workflow. Temporal transparently preserves all the state associated with workflow and its activities.</p>
<p>Below is System architecture of Temporal, more details <a href="https://docs.temporal.io/docs/system-architecture/">here</a>,</p>
<pre><code><figure>
    <img src="https://docs.temporal.io/assets/images/system-architecture-d064cede0c97fc4a86defdc1c9fd020c.png"/> <figcaption>
            <h4>Temporal Architecture</h4>
        </figcaption>
</figure>

</code></pre>
<p>Overall, Temporal offers following features,</p>
<ul>
<li>Workflow implemented as Application code - Basically it allows to implement Workflow as code, just like rest of the codebase of the application. Thus allowing one to concentrate on business logic and reduces complexity about authoring workflow as DSL, JSON etc.</li>
<li>Retries and Timeouts - Nowadays, quite a few steps in workflow involve remote service invocation and whenever one crosses boundary of the application, it is important to have retries and timeouts in place.</li>
<li>Reliability - Robustness against failure</li>
<li>Scalability - Horizontally Scalable</li>
<li>Support for SAGAs - If a Workflow calls multiple external/remote services and if one of them fails then, compensation call to other services will have to be made to complete rollback.</li>
<li>Distributed Cron - Scheduled processing of workflow or steps in workflow.</li>
<li>Persistent Storage in MySQL, Cassandra among others</li>
<li>Frontend for tracking and diagnostics</li>
<li>Monitoring using Prometheus or other backends.</li>
</ul>
<p>It is very easy to get basic &ldquo;Helloworld&rdquo; workflow up and running using detailed instructions on setup provided <a href="https://docs.temporal.io/docs/go-sdk-tutorial-prerequisites">here</a> provided docker desktop or such environment is easily available. Temporal documentation does a great job on this.</p>
<p>To evaluate Temporal further, we will orchestrate below,</p>
<ul>
<li>List of users are imported/received (say from a file or provided as input)</li>
<li>These users are verified/validated by Admin through some Frontend (to simulate a maker/checker process).</li>
</ul>
<p>This may not resemble real world scenario but it will help evaluate features of Temporal like  Signals - Waiting on Events (such as human intervention).</p>
<ul>
<li>
<p>We will have below activities in our workflow,</p>
<ul>
<li>Import users - This activity will import list of users from file/stream. For the sake of simplicity, we will just pass it as string.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">func ImportUsers(ctx context.Context, userdata string, DbConnectionString string) (int, error) {

    logger := activity.GetLogger(ctx)

    logger.Info(&#34;ImportUsers activity started.&#34;, zap.String(&#34;Dbconn&#34;, DbConnectionString))

    // Open connection to database
    db, close, err := GetSQLXConnection(context.Background(), DbConnectionString)
    if err != nil {
        logger.Error(&#34;Cant open connection to database&#34;, zap.Error(err))
        return 0, err
    }

    defer close()

    if _, err := db.Exec(DBSchema); err != nil {
        logger.Error(&#34;Error while executing Schema&#34;, zap.Error(err))
        return 0, err
    }

    logger.Info(&#34;Database connection opened, now parsing user data&#34;)

    sqlStmt := &#34;insert into users(name,dob,city) values(?,?,?)&#34;

    tx := db.MustBegin()

    defer func() {
        if err != nil {
            tx.Rollback()
        }
        tx.Commit()
    }()

    r := csv.NewReader(strings.NewReader(string(userdata)))
    r.Comma = &#39;,&#39;
    r.Comment = &#39;#&#39;

    records, err := r.ReadAll()
    if err != nil {
        logger.Error(&#34;Error while reading&#34;, zap.Error(err))
        return 0, err
    }

    i := 0

    for i, record := range records {
        if i == 0 {
            continue
        }

        logger.Info(&#34;Record read is -&gt;&#34;, record[0])

        if _, err := tx.Exec(sqlStmt, record[0], record[1], record[2]); err != nil {
            logger.Error(&#34;Error while writing user record&#34;, zap.Error(err))
            return 0, err
        }
    }

    return i, nil
}

</code></pre></div><ul>
<li>Approve users - This activity will mark all those users, Approved by Admininstrator via Service, as approved.</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">func ApproveUsers(ctx context.Context, DbConnectionString string, Users string) (int, error) {

    logger := activity.GetLogger(ctx)
    logger.Info(&#34;ApprovedUsers called&#34;, zap.String(&#34;Dbconn&#34;, DbConnectionString), zap.String(&#34;Userlist&#34;, Users))

    db, close, err := GetSQLXConnection(context.Background(), DbConnectionString)
    if err != nil {
        logger.Error(&#34;Cant open connection to database&#34;, zap.Error(err))
        return 0, err
    }

    defer close()

    if _, err := db.Exec(DBSchema); err != nil {
        logger.Error(&#34;Error while executing Schema&#34;, zap.Error(err))
        return 0, err
    }

    r := csv.NewReader(strings.NewReader(Users))

    tx := db.MustBegin()

    defer func() {
        if err != nil {
            tx.Rollback()
        }
        tx.Commit()
    }()

    sqlStmt := &#34;update users set isapproved =1 where id =:1&#34;

    i := 0

    for {
        record, err := r.Read()
        if err == io.EOF {
            break
        }

        if err != nil {
            logger.Error(&#34;Error while reading from file&#34;, zap.Error(err))
            return 0, err
        }

        if i == 0 {
            continue
        }
        i++

        if _, err := tx.Exec(sqlStmt, record[0]); err != nil {
            logger.Error(&#34;Error while writing user record&#34;, zap.Error(err))
            return 0, err

        }
    }
    return i, nil
}
</code></pre></div><ul>
<li>HTTP Service - This service will receive list of approved users and send it over to workflow via Signal,</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">    func (s *server) UpdateUsers(w http.ResponseWriter, r *http.Request, ps httprouter.Params) {

        creader := csv.NewReader(r.Body)
        records, err := creader.ReadAll()
        if err != nil {
            log.Fatal(err.Error())
            http.Error(w, err.Error(), http.StatusBadRequest)
            return
        }

        // Create the client object just once per process
        c, err := client.NewClient(client.Options{})
        if err != nil {
            log.Fatalln(&#34;unable to create Temporal client&#34;, err)
            http.Error(w, &#34;Internal Error :Temporal&#34;, http.StatusInternalServerError)
            return
        }
        defer c.Close()

        workflowOptions := client.StartWorkflowOptions{
            ID:        app.UserApprovalWorkflow,
            TaskQueue: app.UserApprovalTaskQueue,
            RetryPolicy: &amp;temporal.RetryPolicy{
                InitialInterval:    time.Second,
                BackoffCoefficient: 2.0,
                MaximumInterval:    time.Minute,
                MaximumAttempts:    5,
            },
        }

        _, err = c.SignalWithStartWorkflow(r.Context(), app.UserApprovalWorkflow, app.ApprovalSignalName,
            records, workflowOptions, app.OnboardUsers, app.Userdata, s.DBConnection)

        if err != nil {
            log.Fatal(err.Error())
            http.Error(w, &#34;Internal Error: Workflow&#34;, http.StatusInternalServerError)
            return
        }

        fmt.Fprint(w, &#34;Success&#34;)
}

</code></pre></div><ul>
<li>HTTP service uses <code>workflow.SignalWithStartWorkflow</code> function. This function sends the signal to running instance of workflow or starts new if none is in progress.</li>
</ul>
</li>
</ul>
<p>Full source code is available <a href="https://github.com/sachinsu/temporalevaluation">here</a></p>
<p>Temporal documentation has reference to Helm charts for deploying temporal in clustered configuration, for organization who is managing own data center it would be interesting to know if it also supports bare metal based deployment in addition to Kubernetes. Will update this post as and when details are available on this.</p>
<p>Overall, Temporal provides a different approach to workflow orchestration. Its been battle tested at <a href="https://uber.com">Uber</a> and host of other companies. Temporal <a href="community.temporal.io">Community</a> is a very active one with founders actively participating in discussions.</p>
<ul>
<li><a href="https://github.com/firdaus/awesome-cadence-temporal-workflow">Collection of Temporal related stuff</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Getting Started with OpenTelemetry</title><link>https://sachinsu.github.io/posts/opentelemetry/</link><pubDate>Sat, 07 Nov 2020 08:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/opentelemetry/</guid><description>Background How many times have we landed up in a meeting staring at random slowness or such production issues in a distributed Application ? only to experience helplessness with limited (or often times no) visibility available about the runtime behavior of the Application. It often ends up in manually correlating whatever diagnostic data available from Application and combining it with trace/logs that are available from O/S, databases etc. and trying to figure out &amp;ldquo;Root cause&amp;rdquo; of the issue.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>How many times have we landed up in a meeting staring at random slowness or such production issues in a distributed Application ? only to experience helplessness with limited (or often times no) visibility available about the runtime behavior of the Application. It often ends up in manually correlating whatever diagnostic data available from Application and combining it with  trace/logs that are available from O/S, databases etc. and trying to figure out &ldquo;Root cause&rdquo; of the issue.</p>
<p>Today’s mission critical, distributed applications and systems make it even more important to <strong>observe</strong> them, be it serving web requests, processing stream of data or handling events. The scale at which these applications/systems operate at, often hundreds or thousands of requests, requires <strong>watching</strong> how well system is working, instead of waiting for failure or doing analysis post failure.</p>
<p>In distributed systems, telemetry can be divided into three overarching flavors:</p>
<ul>
<li><strong>(Distributed) Traces</strong>: detailed records of the paths that distinct requests take as they propagate across an entire system (including across service boundaries)</li>
<li><strong>Metrics</strong>: aggregate statistics (mostly counters and gauges) about processes and infrastructure, typically with key:value tags attached to disaggregate patterns</li>
<li><strong>Logs</strong>: timestamped messages – sometimes structured – emitted by services or other components (though, unlike traces, not necessarily associated with any particular user request or transaction)</li>
</ul>
<p>To this effect, Cloud Native Computing Foundation (<a href="https://cncf.io">CNCF</a>) has been working on Opentelemetry.</p>
<h2 id="what-is-opentelemetry">What is OpenTelemetry?</h2>
<blockquote>
<p>OpenTelemetry is a vendor-neutral standard for collecting telemetry data for applications, their supporting infrastructures, and services.</p>
</blockquote>
<p>For deep dive, history etc., refer to Overview <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/logs/overview.md">here</a>.</p>
<p>So is this standard already available? As of this writing, it is about to go <strong>GA</strong> soon. This makes it more important to be aware of its scope (subjected to change). Let&rsquo;s see how it is proposing to address/implement Observability.</p>
<p>Below diagram depicts what OpenTelemetry does in Nutshell (Source: <a href="www.appdynamics.com">AppDynamics</a>),</p>
<figure>
    <img src="https://www.appdynamics.com/wp-content/uploads/2020/10/17869_AWS_announce_OT_img2_C2-1.jpg"/> <figcaption>
            <h4>OpenTelemetry in Nutshell</h4>
        </figcaption>
</figure>

<p>The general process of using OpenTelemetry is,</p>
<ul>
<li>Instrumentation of Application Code (including libraries)</li>
<li>Validate Instrumentation by sending it to Collector like Jaeger (For  simplicity, we will only be using Console exporter)</li>
<li>Learn how Instrumentation helps in correlating, watching runtime behavior</li>
</ul>
<p>While vendors, having back-end systems, are providing or working on integrations with OpenTelemetry. The OpenTelemetry team has provided client libraries for Instrumentation in Go, .NET, Java,JavaScript, Python (and more coming). So lets us see what these libraries offer as of today by implementing .NET library.</p>
<p>In this post, We will look at how Opentelemetry helps us with &ldquo;Distributed tracing&rdquo;.</p>
<h3 id="opentelemetry-for-net">OpenTelemetry for .NET</h3>
<p><a href="https://dot.net">.NET</a> client of OpenTelemetry supports both <a href="https://dotnet.microsoft.com/download/dotnet-framework">.NET Framework</a> as well as <a href="https://dotnet.microsoft.com/download/dotnet-core">.NET Core</a>.</p>
<p>For list of available instrumentation libraries and exporters, refer <a href="https://github.com/open-telemetry/opentelemetry-dotnet">here</a></p>
<p>In the below sections, We will try to simulate a scenario, which is typical in Microservices style of Architecture, where service invokes another service using HTTP.  Now, aim is to verify how using OpenTelemetry will help in watching traffic  between these two services.</p>
<figure>
    <img src="/images/ot_scenario.svg"/> <figcaption>
            <h4>Sample Scenario</h4>
        </figcaption>
</figure>

<p>Lets start,</p>
<ul>
<li>
<p>Create Web Service (I am using .NET Core SDK 3.1.300 on Windows)</p>
<ul>
<li>
<p>Use <code>dotnet new webapi</code> to scaffold a REST API</p>
</li>
<li>
<p>Add references to below packages using Nuget,</p>
<ul>
<li><code>OpenTelemetry.Exporter.Console</code> - Exporter package to output telemetry to Console</li>
<li><code>OpenTelemetry.Instrumentation.AspNetCore</code> - Package that transparently instruments ASP.NET Core request processing pipeline</li>
<li><code>OpenTelemetry.Instrumentation.Http</code> - Package that transparently instruments HTTP Communication.</li>
</ul>
</li>
<li>
<p><code>Startup.cs</code> - It configures OpenTelemetry instrumentation with Console Exporter and instrumentation for HTTP requests. Below is ConfigServices function of Startup class.</p>
<pre><code>  public void ConfigureServices(IServiceCollection services)
  {
      services.AddOpenTelemetryTracing(
              (builder) =&gt; builder.AddAspNetCoreInstrumentation(opt =&gt; opt.Enrich
                      = (activity, eventName, rawObject) =&gt;
                  {
                      if (eventName.Equals(&quot;OnStartActivity&quot;))
                      {
                          if (rawObject is HttpRequest httpRequest)
                          {
                              activity.SetTag(&quot;requestProtocol&quot;, httpRequest.Protocol);
                          }
                      }
                      else if (eventName.Equals(&quot;OnStopActivity&quot;))
                      {
                          if (rawObject is HttpResponse httpResponse)
                          {
                              activity.SetTag(&quot;responseLength&quot;, httpResponse.ContentLength);
                          }
                      }
                  })
                  .AddHttpClientInstrumentation()
                  .AddConsoleExporter()   //opt =&gt; opt.DisplayAsJson = true)
          );

  }
</code></pre>
</li>
<li>
<p><code>WeatherForecastController.cs</code> - This is default controller added by <code>dotnet new webapi</code> command. We will add <code>GET</code> endpoint to simulate a dummy HTTP Request. This is to verify telemetry produced for the same.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">[HttpGet(&#34;{key}&#34;)]
public async Task&lt;IEnumerable&lt;WeatherForecast&gt;&gt; Get(string key)
{
    // Making an http call here to serve as an example of
    // how dependency calls will be captured and treated
    // automatically as child of incoming request.
    var res = await httpClient.GetStringAsync(string.Format(&#34;https://www.google.com/search?q={0}&#34;, key));
    var rng = new Random();
    return Enumerable.Range(1, 5).Select(index =&gt; new WeatherForecast
    {
        Date = DateTime.Now.AddDays(index),
        TemperatureC = rng.Next(-20, 55),
        Summary = Summaries[rng.Next(Summaries.Length)],
    })
    .ToArray();
}
</code></pre></div></li>
</ul>
</li>
<li>
<p>Lets create a Service 1
For the sake of simplicity, we will have &ldquo;Service 1&rdquo; implemented as Console Application,</p>
<ul>
<li>
<p>Use <code>dotnet new console</code> to create new App.</p>
</li>
<li>
<p>Add reference to &ldquo;OpenTelemetry.Exporter.Console&rdquo;  using <code>dotnet add OpenTelemetry.Exporter.Console -version 0.7.0-beta.1</code>. This package is specifically meant for exporting telemetry to Console. There are other exporters available to export to Jaegar, Zipkin etc. but this is simplest one to setup.</p>
</li>
<li>
<p>Add reference to &ldquo;OpenTelemetry.Instrumentation.Http&rdquo;  using <code>dotnet add OpenTelemetry.Instrumentation.Http -version 0.7.0-beta.1</code>. This package helps in instrumenting HTTP requests.</p>
</li>
<li>
<p>Add below code to <code>program.cs</code>,</p>
<p>static async Task Main(string[] args)
{
// Configure OpenTelemetry Tracer with Console exported and initiate it
Sdk.CreateTracerProviderBuilder()
.AddHttpClientInstrumentation()
.AddConsoleExporter()
.Build();</p>
<pre><code>          try
          {
              // Simulate HTTP Request to our service
              string responseBody = await client.GetStringAsync(&quot;https://localhost:5001/weatherforecast/abc&quot;);

              Console.WriteLine(responseBody);

          }
          catch (HttpRequestException e)
          {
              Console.WriteLine(&quot;\nException Caught!&quot;);
              Console.WriteLine(&quot;Message :{0} &quot;, e.Message);
          }

          Console.WriteLine(&quot;Done!&quot;);
      }
</code></pre>
</li>
</ul>
<p>In this class, we have configured  OpenTelemetry tracer with Console Exported and intialized it. Further, HTTP requests are automatically instrumented since we have added <code>OpenTelemetry.Instrumentation.Http</code> package.</p>
</li>
<li>
<p>Observe the Telemetry,</p>
<ul>
<li>Start the Web Service. Check that it is listening on port <code>8080</code> by visiting <code>https://localhost:8080</code>. Note: you may have to install Client certificate to enable SSL.</li>
<li>Start the Console Application. This application will send HTTP request to the service.</li>
<li>Observe the telemetry produced by,
<ul>
<li>
<p>Console Application,</p>
<figure>
    <img src="/images/ot_client.png"/> <figcaption>
            <h4>Default Telemetry generated</h4>
        </figcaption>
</figure>

<ul>
<li>Activity Id (GUID) is generated for a Span (Refer <a href="https://github.com/open-telemetry/opentelemetry-specification/blob/master/specification/overview.md#trace">here</a> for details on what span means)</li>
<li>It also records start and end time</li>
</ul>
</li>
<li>
<p>Web Service,
<figure>
    <img src="/images/ot_service.png"/> <figcaption>
            <h4>Default Telemetry</h4>
        </figcaption>
</figure>
</p>
</li>
<li>
<p>Observations</p>
<ul>
<li>Check Activity ID being shown is same as one reported by Console Application. So correlation has been established across process boundaries. This is important when tracing end to end across processes. This is achieved by means of passing Activity ID as HTTP Header.  In a Visualization tool, this correlation is used to depict end to end flow with time at each step.</li>
<li>By default, it logs start and end time. For any HTTP request, it generates additional telemetry covering URL to which request was sent and start and end time.</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>In Summary, this default telemetry can obviously be enhanced by adding Tags. When coupled with additional telemetry in the form of metering (to statistically observe behavior of high traffic, large scale system) and telemetry from Infrastructure (i.e. OS) and other Systems (e.g. Databases), it truly provides complete view of proceedings end to end.</p>
<p>Hope this provides overview of instrumentation as provided by OpenTelemetry. Let me know if you have any questions or suggestions in Comments section below.</p>
<p>Instrumenting .NET framework based Apps for same scenario is similar to above, refer folder <code>Opentelemetry</code> in repository <a href="https://github.com/sachinsu/opentelemetrydotnet">here</a></p>
<h3 id="useful-references">Useful References,</h3>
<ul>
<li><a href="https://otel.lightstep.com">What is OpenTelemetry?</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Ninja - Using lightweight build system for Go projects</title><link>https://sachinsu.github.io/posts/ninjabuildsystem/</link><pubDate>Tue, 27 Oct 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/ninjabuildsystem/</guid><description>Background I primarily work on Windows for development purposes. Whenever its about writing code in Golang, invariably one comes across usage of Make. A quick check on popular Go projects on Github will show Makefile being used to automate tasks like linting, build, testing and deployment.
Being on Windows, i have been looking for alternative build tool that is easy to setup (i.e. doesn&amp;rsquo;t require mingw and such environments) and use compared to Make (which is primarily targetted at Unix and Unix like Operating Systems).</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>I primarily work on Windows for development purposes. Whenever its about writing code in Golang, invariably one comes across usage of Make. A quick check on popular Go projects on Github will show Makefile being used to automate tasks like linting, build, testing and deployment.</p>
<p>Being on Windows, i have been looking for alternative build tool that is easy to setup (i.e. doesn&rsquo;t require mingw and such environments) and use compared to Make (which is primarily targetted at Unix and Unix like Operating Systems).</p>
<p>Following a wonderful post by Julia Evans (read <a href="https://jvns.ca/blog/2020/10/26/ninja--a-simple-way-to-do-builds/">here</a>) on <a href="https://ninja-build.org">Ninja</a>. I decided to give it a try for a Golang Application.</p>
<p>Julia, in her post, has covered important aspects of Ninja but to summarize, Ninja is,</p>
<ul>
<li>A build automation tool</li>
<li>Lightweight, with focus on speed</li>
<li>Easy to configure</li>
<li>Cross platform (Easy to setup across Windows and Linux)</li>
</ul>
<p>With this, lets give it a try,</p>
<p>To start with, lets create a simple go &lsquo;Hello World&rsquo; project,</p>
<ul>
<li>
<p>Initiate Go Module (in a Empty folder), <code>go mod init github.com/sachinsu/ninjabuild</code></p>
</li>
<li>
<p>Create a &lsquo;main.go&rsquo; that prints &lsquo;Hello World&rsquo;,</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-go" data-lang="go">        <span style="color:#f92672">package</span> <span style="color:#a6e22e">main</span>

        <span style="color:#f92672">import</span> <span style="color:#e6db74">&#34;fmt&#34;</span>

        <span style="color:#66d9ef">func</span> <span style="color:#a6e22e">main</span>() {
            <span style="color:#a6e22e">fmt</span>.<span style="color:#a6e22e">Println</span>(<span style="color:#e6db74">&#34;hello world&#34;</span>)
        }
</code></pre></div><ul>
<li>
<p>Now setup Ninja, It is as easy as downloading binary for your Platform. It is also possible to build it locally, if you prefer it that way. For details, refer <a href="https://ninja-build.org/">here</a></p>
</li>
<li>
<p>Once ninja is setup, lets create build configuration file (i.e. build.ninja),</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-YAML" data-lang="YAML"><span style="color:#ae81ff">GOARCH = amd64</span>
<span style="color:#ae81ff">GOOS = linux</span>

<span style="color:#ae81ff">rule lint</span>
 <span style="color:#ae81ff">command = go vet -mod=vendor ./...</span>

<span style="color:#f92672">build lintoutput</span>: <span style="color:#ae81ff">lint </span>

<span style="color:#ae81ff">rule unit</span>
 <span style="color:#ae81ff">command = go test -mod=vendor -cover -v -short ./...</span>

<span style="color:#f92672">build utest</span>: <span style="color:#ae81ff">unit </span>


<span style="color:#ae81ff">rule compile</span>
 <span style="color:#ae81ff">command = cmd /c go mod tidy &amp;&amp; go mod vendor &amp;&amp;  go build -o $out $in  &amp;&amp; echo &#34;build done.&#34;</span>
 <span style="color:#ae81ff">description = compile $in</span>

<span style="color:#f92672">build ninjabuild.exe</span>: <span style="color:#ae81ff">compile .</span>
</code></pre></div><p>lets go through the contents of this file,</p>
<ul>
<li>
<p>One can define variables <code>GOARCH = amd64</code> and refer them as <code>$GOARCH</code></p>
</li>
<li>
<p>Ninja configuration is combination of build step and associated rule, for e.g.</p>
</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-YAML" data-lang="YAML"><span style="color:#ae81ff">rule compile</span>
 <span style="color:#ae81ff">command = cmd /c go mod tidy &amp;&amp; go mod vendor &amp;&amp;  go build -o $out $in  &amp;&amp; echo &#34;build done.&#34;</span>
 <span style="color:#ae81ff">description = compile $in</span>

<span style="color:#f92672">build ninjabuild.exe</span>: <span style="color:#ae81ff">compile .</span>
</code></pre></div><ul>
<li>Above snippet, defines rule <code>compile</code> with associated command that builds the code. Being on Windows, i have used <code>cmd /c</code> to start a new shell and concatenate multiple commands as part of <code>compile </code>rule using <code>&amp;&amp;</code> which chains the commands and executes next one only if current one succeeds.</li>
</ul>
<p>As demonstrated in above file, Ninja can be used to automate wide variety of tasks like build, tests, deployment and so on.</p>
<p>Many of you using Make will find the approach similar to it. In contrast to Make, Ninja lacks features such as string manipulation, as Ninja build files are not meant to be written by hand. Instead, a &ldquo;build generator&rdquo; should be used to generate Ninja build files.</p>
<p>I found simplicity (of installation and configuration) and easy of use to be  key aspects of this tool.</p>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Validating urls from 'Useful Links' section using bash / command line tools</title><link>https://sachinsu.github.io/posts/urlhealthchecks/</link><pubDate>Thu, 15 Oct 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/urlhealthchecks/</guid><description>Background I started this blog, https://sachinsu.github.io few months back .
In this relatively short period of time, Blog has sizeable number of useful links across various categories in addition to the detailed blog post like this one.
As an ongoing activity, I think that it is necessary to verify links mentioned on this blog.
So how can it be done ? obviously one way is to do it manually by visiting each link and updating/removing those that are no longer available.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>I started this blog, <a href="https://sachinsu.github.io">https://sachinsu.github.io</a> few months back .</p>
<p>In this relatively short period of time, Blog has sizeable number of useful links across various categories in addition to the detailed blog post like this one.</p>
<p>As an ongoing activity, I think that it is necessary to verify links mentioned on this blog.</p>
<p>So how can it be done ? obviously one way is to do it manually by visiting each link and updating/removing those that are no longer available. but there is always of better way of doing things.</p>
<p>The requirement is to,</p>
<ul>
<li>Parse all the files to links (being in Markdown links will be enclosed in brackets)</li>
<li>Send request to each link and verify if its active using HTTP Status (say 200 or 302)</li>
</ul>
<h2 id="approach">Approach</h2>
<p>Enter Automation !!</p>
<p>It is possible to write a utility/tool (or it might be already available) or can good old command line utlities be used for this task?</p>
<p>I decided to go for dos / shell script way and surprisingly all the necessary tools are already available.</p>
<p>Below is single command line that fulfils the requirement,</p>
<p><code>grep -E -i -w &quot;http|https&quot; *.md | sed 's/](http/\nhttp/g' | sed 's/)/\n/g' | grep ^http | xargs curl -s -I  -w 'URL:%{url_effective} - %{http_code}\n' | grep ^URL:</code></p>
<p>In above chain,</p>
<ul>
<li>
<p>I am using excellent <a href="https://cmder.net/">Cmder</a> console emulator, which also makes above nice tools (grep, sed etc.) available on Windows.</p>
</li>
<li>
<p><a href="https://man7.org/linux/man-pages/man1/grep.1.html">grep</a> -E -i -w &ldquo;http|https&rdquo; *.md  - this command extracts all the lines containing <code>http(s)</code> from all the markdown (.md) files</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Pipeline_(Unix)">Pipe |</a> - Pipe command streams output of command to the next one.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Sed">sed</a> &rsquo;s/](http/\nhttp/g' - this sed (stream editor) command adds line break before <code>http</code> for better extraction.</p>
</li>
<li>
<p><a href="https://en.wikipedia.org/wiki/Sed">sed</a> &rsquo;s/)/\n/g' - this sed (stream editor) command removes trailing <code>)</code> bracket.</p>
</li>
<li>
<p><a href="https://man7.org/linux/man-pages/man1/grep.1.html">grep</a> ^http  - this command removes all lines not containing <code>http</code>.</p>
</li>
<li>
<p><a href="https://man7.org/linux/man-pages/man1/xargs.1.html">xargs</a> - xargs is a command on Unix and most Unix-like operating systems used to build and execute commands from standard input.</p>
</li>
<li>
<p><a href="https://curl.haxx.se/">curl</a> -s -I  -w &lsquo;URL:%{url_effective} &mdash;&gt; %{http_code}&rsquo;' - previously used <code>xargs</code> command feeds each line (url) to this command as last argument. This command sends tcp request to the URL and prints out http status code along with URL.</p>
</li>
<li>
<p><a href="https://man7.org/linux/man-pages/man1/grep.1.html">grep</a> ^URL: - For some reason, CURL outputs content even if <code>-s</code> (silent) parameter is passed. Hence, this grep command is used to ignore all lines not containing URL and HTTP Status.</p>
</li>
</ul>
<p>The output is as below,</p>
<figure>
    <img src="/images/urloutput.png"/> <figcaption>
            <h4>List of URLs with HTTP Status code</h4>
        </figcaption>
</figure>

<p>So, It is  possible to quickly come up with this using built-in tools if writing a program is not an option or cumbersome for task at hand.</p>
<p>As a next step, Plan is to automatically run this script as part of Github Build and notify in case of any URL is failing so that appropriate action can be taken.</p>
<p>Let me know (in comments) if you are aware of any alternate better way of achieving this.</p>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Trobleshooting TCP Connection request time outs</title><link>https://sachinsu.github.io/posts/connectiontimeouts/</link><pubDate>Tue, 25 Aug 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/connectiontimeouts/</guid><description>Background I recently had opportunity to support team who has been battling with Intermittent (scary i know :)) issues with TCP connectivity in Production.
Simplified deployment Architecture is as below,
High Level Architecture Technology Stack used is Microsoft .NET Framework 4.8 using ODP.NET for Oracle Connectivity (Oracle Server is 8 CPU box). Each of Web Servers in cluster have IIS hosted on it with multiple Applications (Application domains) serving HTTP(s) based traffic.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>I recently had opportunity to support team who has been battling with Intermittent (scary i know :)) issues with TCP connectivity in Production.</p>
<p>Simplified deployment Architecture is as below,</p>
<figure>
    <img src="/images/conntimeoutarch.png"/> <figcaption>
            <h4>High Level Architecture</h4>
        </figcaption>
</figure>

<p>Technology Stack used is Microsoft .NET Framework 4.8 using ODP.NET for Oracle Connectivity (Oracle Server is 8 CPU box). Each of Web Servers in cluster have IIS hosted on it with multiple Applications (Application domains) serving HTTP(s) based traffic. These applications connect to Oracle Database.</p>
<p>Team is experienced in developing and running .NET Apps, but they needed help to diagnose and fix &ldquo;Connection request timed out&rdquo; exceptions being thrown while connecting to backend database.</p>
<h2 id="problem-statement">Problem Statement</h2>
<p>Host of .NET Applications (Web Applications, Web APIs) connect to Oracle Database. Each of them use <a href="https://www.oracle.com/database/technologies/appdev/dotnet/odp.html">ODP.NET</a>. ODP.NET maintains connection pool per Application domain (Database resident Connection pool is not used). Some of these applications receive high number of requests per second compared to others.</p>
<p><code>Oracle.ManagedDataAccess.Client.OracleException (0x80004005): Connection request timed out....</code> has been reported randomly which results in failure of business transactions. ODP.NET provides extensive trace and along with above trace also contains <code>OracleInternal.Network.NetworkException (0x80004005): Network Transport: TCP transport address connect failure ---&gt; System.Net.Sockets.SocketException (0x80004005): A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond</code></p>
<p>Specifically, Applications, receiving less traffic, were reporting it often compared to those with high traffic.</p>
<h2 id="approach">Approach</h2>
<ul>
<li>Simulate the Exception in Test Environment - We decided to try and simulate this exception in a Test Environment. Test environment is scaled down (50%) compared to production. Random Exceptions in production could not be simulated due to lack of Test Automation. for example, In Production, each server receives traffic for multiple HTTP End points whereas in Test environment, load testing was being done only against Single Application.</li>
</ul>
<p>This was like a end of the road since simulation would have greatly helped in diagnosing the issue. However, the show must go on so we decided to,</p>
<ul>
<li>
<p>Check online documentation regarding this exception,</p>
<ul>
<li>
<p>&ldquo;Pooled&rdquo; or &ldquo;Non-pooled&rdquo; Connection - Whenever, ODP.NET raises &ldquo;..timed out&rdquo; error, it diffrentiates the same to indicate whether error is due to delay in retrieving connection from pool or if it is due to delay from the database server (Ref: <a href="https://docs.oracle.com/en/database/oracle/oracle-data-access-components/19.3/odpnt/featConnecting.html#GUID-1152D13D-464C-4FE7-9949-32FCA9572B59">Here</a>). From this, it was clear that issue is clearly due to delay in obtaining response from database server.While this was happening , Database server (8 CPU Core box) was reporting less than 50% CPU Usage but it still had large number of inactive sessions originated from IIS Servers.</p>
</li>
<li>
<p>Since the exception was reported frequently in low traffic applications, it was decided to track and verify the same on firewall and database server,</p>
<ul>
<li>
<p>Firewall - Firewall had TCP Timeout of 30 minutes and  maintains detailed log of sessions. Quick analysis of it revealed that,</p>
<ul>
<li>Production Environment - Unusually high number of sessions were being destroyed due to &ldquo;Age out&rdquo; (i.e. time out)</li>
<li>Test Environment - No abnormal activity was reported. Most probably because of differences in traffic.</li>
</ul>
</li>
<li>
<p>Database Server - Listener Log on Oracle Database server did not had any log entry for request at the precise time when Application(s) had reported Exception.</p>
</li>
</ul>
</li>
<li>
<p>Next is to check settings in Application for connectivity with Oracle. Though ODP.NET does not have any direct  &ldquo;Time out&rdquo; or &ldquo;Time to live&rdquo; settings, it does provide few parameters that can influence it,</p>
<ul>
<li>&ldquo;Connection Lifetime&rdquo; - ODP.NET uses this whenever Connection is closed and disposed by the Application. It will be destroyed (after maintaining number of connections as per &ldquo;Min Pool Size&rdquo;)  if it has exceeded life time. For whatever reasons, this was set to unusually high duration (i.e. 90000 seconds).</li>
<li>&ldquo;Connection Timeout&rdquo; - Period for which ODP.NET waits for the connection to be made available. This was set to 60 Seconds.</li>
</ul>
</li>
<li>
<p>Oracle has articles titled &ldquo;ODP-1000 &ldquo;Connection Request Timed Out&rdquo; Explained&rdquo; and &ldquo;Resolving Problems with Connection Idle Timeout With Firewall (Doc ID 257650.1)&rdquo; where it primarily recommends measures for tuning Application as well as database.</p>
</li>
</ul>
<p>On the basis of above, it was decided to modify the code for below,</p>
<ul>
<li>Thorough code review to verify that every ODP.NET Project is closed/disposed.</li>
<li>Upgrade ODP.NET to latest version (v19.8.0 as of this writing)</li>
<li>Turn &ldquo;KeepAlive&rdquo; while connecting to database</li>
<li>Leverage ODP.NET tracing in case of exception</li>
<li>Modify the connection lifetime to be less than time out at firewall and increase the &ldquo;Time out&rdquo; period.</li>
<li>Introduce Retry functionality using Excellent <a href="https://github.com/App-vNext/Polly">Polly</a> library with exponential back-off.</li>
</ul>
<p><code>
<iframe src="//carbon.now.sh/embed/d3888ab4418b937a650e880ec4682653?filename=extensions.cs"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>
</code></p>
<p>These changes have been deployed to production and so far % of &ldquo;Connection Request Timed out&rdquo; errors have gone down significantly.</p>
</li>
</ul>
<h2 id="wrap-up">Wrap up</h2>
<p>Some key areas of focus are,</p>
<ul>
<li>For a distributed system, Always validate assumptions by dignosing end to end.</li>
<li>Plan to have test automation readyness to simulate production like load.</li>
<li>Monitoring the behavior end to end using logs.</li>
<li>Currently, Pool settings across applications is not optimal going by Oracle Real world Guidelines, also be mindful of <a href="https://docs.oracle.com/en/database/oracle/oracle-database/18/adfns/connection_strategies.html">Connection Storms</a></li>
</ul>
<p>Happy Troubleshooting !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Tool to mass DM followers on Twitter in Go</title><link>https://sachinsu.github.io/posts/massdmgolang/</link><pubDate>Sat, 25 Jul 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/massdmgolang/</guid><description>Background I recently came across bounty by Balaji Srinivasan to send Direct Message to all twitter followers. Currently, i do not intend to participate in bounty and this is mere exercise.
This is an attempt to write CLI tool in Golang in response to it.
For detailed requirements, refer here
Approach In Brief,
CLI should,
accept arguments like Twitter API Key,Auth token, DM Message Download all followers (with profile details) Rank them by Criteria (e.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>I recently came across bounty by <a href="https://twitter.com/balajis/status/1271945241881268224?s=20">Balaji Srinivasan</a> to send Direct Message to all twitter followers. <em>Currently, i do not intend to participate in bounty and this is mere exercise.</em></p>
<p>This is an attempt to write CLI tool in <a href="https://golang.org">Golang</a> in response to it.</p>
<p>For detailed requirements, refer <a href="https://github.com/balajis/twitter-export">here</a></p>
<h2 id="approach">Approach</h2>
<p>In Brief,</p>
<ul>
<li>
<p>CLI should,</p>
<ul>
<li>accept arguments like Twitter API Key,Auth token, DM Message</li>
<li>Download all followers (with profile details)</li>
<li>Rank them by Criteria (e.g. Location)</li>
<li>Send each follower a DM with provided message (upto daily DM Limit)</li>
<li>be easy to use and maintain</li>
</ul>
</li>
<li>
<p>Notes,</p>
<ul>
<li>Due to Daily DM Limit, Follower details will have to be persisted alongside flag indicating if DM has been sent. SQLIte is used from simplicity perspective.</li>
<li>There should be a scheduled job that will send the DM upto daily DM Limit. At the same time, it needs to refetch any new followers and push them in the flow (reconcile).</li>
<li>Potentially, this could be extended to other social media providers other than twitter.</li>
<li>Milestones,
<ul>
<li>Create code structure
<ul>
<li>Plan is to have separation between CLI &amp; have twitter as go package</li>
</ul>
</li>
<li>Accept Arguments and Connect to Twitter</li>
<li>Study and complete follower retrieval</li>
<li>Ranking of followers</li>
<li>Persisting followers</li>
<li>Sending DM upto Daily limit</li>
</ul>
</li>
<li>Rules,
<ul>
<li>Use golang&rsquo;s in-built packages as much as possible</li>
<li>Every milestone to have associated Unit test cases</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="current-status">Current Status</h2>
<p>The code is ready and functionality to retrieve followers and saving in local DB is tested. Code to send DM is not yet tested as it will require setting up dummy twitter account.</p>
<h2 id="roadmap">Roadmap</h2>
<ul>
<li>In addition to CLI, Expose the utility as responsive Web Application</li>
<li>Possibly extend this to social media platforms other than <a href="https://twitter.com">Twitter</a></li>
</ul>
<p>Have a look at code on <a href="https://github.com/sachinsu/twitterexport">Github</a> and let me know what you think.</p>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Web Security Measures in ASP.NET Applications</title><link>https://sachinsu.github.io/posts/websecurity/</link><pubDate>Thu, 04 Jun 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/websecurity/</guid><description>At my current workplace, All Applications are expected to adhere to PCI DSS standards meant for Data protection, Access Regulation and so on. Dedicated SOC Team,consisting of Security analyst who are continously on the prawl to identify breach, conduct periodic auditing of Applications, hardening of Servers.
While all our .NET applications adhere to below guidelines,
ASP.NET Security Overview Secure Coding Guidelines Security Guidelines by OWASP We also use tools like Snyk to perform code vulnerability analysis as part of Jenkins driven CI/CD pipeline.</description><content type="html"><![CDATA[<p>At my current workplace, All Applications are expected to adhere to  <a href="https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard">PCI DSS standards</a> meant for Data protection, Access Regulation and so on.  Dedicated <a href="https://en.wikipedia.org/wiki/Information_security_operations_center">SOC</a> Team,consisting of Security analyst who are continously on the prawl to identify breach, conduct periodic auditing of Applications, hardening of Servers.</p>
<p>While all our .NET applications adhere to below guidelines,</p>
<ul>
<li><a href="https://support.microsoft.com/en-in/help/891028/asp-net-security-overview">ASP.NET Security Overview</a></li>
<li><a href="https://docs.microsoft.com/en-us/dotnet/standard/security/secure-coding-guidelines">Secure Coding Guidelines</a></li>
<li><a href="https://cheatsheetseries.owasp.org/cheatsheets/DotNet_Security_Cheat_Sheet.html">Security Guidelines by OWASP</a></li>
</ul>
<p>We also use tools like <a href="https://www.snyk.io/">Snyk</a> to perform code vulnerability analysis as part of Jenkins driven CI/CD pipeline. In spite of above, we do come across  vulnerabilities identified by SOC Team which we needs to be addressed quickly. SOC team uses tools such as <a href="https://portswigger.net/burp">Burp Suite</a>.</p>
<p>This post is going to summarize such incidents reported so far (and will keep updating it). In most of these cases, These issues required additional efforts over and above those provided by library or framework. Hopefully, it will be helpful to anyone trying address such vulnerabilities.</p>
<h3 id="cookie-path">Cookie Path</h3>
<p>Every cookie being sent by the Web application has attributes like,</p>
<ul>
<li>HTTPOnly - Indicates whether a cookie is accessible by client-side script</li>
<li>Domain - Indicates the domain to associate the cookie with</li>
<li>Path - the virtual path to transmit with the current cookie</li>
<li>Secure - Indicates whether cookie is sent only on Secure (HTTPS) Connections.</li>
</ul>
<p>OWASP has nice primer on <a href="https://owasp.org/www-chapter-london/assets/slides/OWASPLondon20171130_Cookie_Security_Myths_Misconceptions_David_Johansson.pdf">Cookie Security</a>.</p>
<p>Of the above, <strong>Path</strong> attribute limits the scope of a cookie to a specific path on the server and can therefore be used to prevent unauthorized access to it from other applications on the same host.  Accordingly, SOC Team had recommended that all cookies issued by application must have path attribute set.</p>
<p>In case of typical ASP.NET Application, there are cookies generated by .NET framework (like for Session, Anti XSRF Token and son on) and custom ones which are issued and used by Application itself.</p>
<p>While it is fairly easy to set path for custom ones, we had to make code changes for cookies issued by .NET framework libraries. Lets take case of <a href="https://docs.microsoft.com/en-us/previous-versions/iis/6.0-sdk/ms525506(v%3Dvs.90)">Session ID</a> cookie, by default, this cookie always has root <code>/</code> as path. So how can this be changed as per the application&rsquo;s deployment settings (i.e. specific virtual directory in IIS)?</p>
<p>We tried below,</p>
<ul>
<li>Step 1, try using <a href="https://docs.microsoft.com/en-us/dotnet/api/system.web.configuration.httpcookiessection?view=netframework-4.8">httpcookies</a> section in web.config like,</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">    &lt;httpCookies requireSSL=&#34;false&#34; httpOnlyCookies=&#34;true&#34;  domain=&#34;site.com/myapp&#34;/&gt;
</code></pre></div><p>First of all, this configuration element does not allow setting <strong>Path</strong> property and even during runtime, only the <strong>Domain</strong> property is populated while issuing the cookie. So this definitely does not help address the issue.</p>
<p>So other way is to programmatically set the path for Session Cookie. This can be done by providing custom implementation of SessionIDManager class like below,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">  public class MySessionIDManager : SessionIDManager, ISessionIDManager
    {
        void ISessionIDManager.SaveSessionID(HttpContext context, string id, out bool redirected, out bool cookieAdded)
        {
            base.SaveSessionID(context, id, out redirected, out cookieAdded);

            if (cookieAdded)
            {
                var name = &#34;ASP.NET_SessionId&#34;;
                var cookie = context.Response.Cookies[name];
                // this will be possibly read from configuration
                cookie.Path = &#34;/myapp&#34;; 
           }
        }
    }

</code></pre></div><p>Thanks to <a href="https://stackoverflow.com/questions/2326521/asp-net-session-cookies-specifying-the-base-domain">this</a> Stackoverflow thread for listing this approach. Application under consideration only had this particular cookie however, for all other .NET framework issued cookies, similar technique will have to be used.</p>
<h3 id="samesite-cookie">SameSite Cookie</h3>
<p>This is already detailed <a href="/posts/samesitecookies">here</a></p>
<h3 id="masking-of-sensitive-data">Masking of Sensitive Data</h3>
<p>This typically involves masking the sensitive data like</p>
<ul>
<li>E-mail id</li>
<li>Phone Number</li>
<li>Credit/Debit Card Number</li>
</ul>
<p>It could well be used in Web Application or be received or sent as part of HTTP API.</p>
<p>The best bet in this case is to mask it on the server side itself before sending/rendering the data in browser. Note that, in some cases, above fields are used for data binding purposes. The approach we followed in such scenario was to use Hash value of it instead of merely masking the data. We have always used <a href="https://docs.microsoft.com/en-us/dotnet/api/system.security.cryptography.sha256?view=netcore-3.1">SHA256</a> or above for hashing.</p>
<h2 id="summary">Summary</h2>
<p>Addressing security vulnerabilities is continuous process as hackers keep on inventing new ways for breaching and exploiting weak spots. As Application Architect/Developers, we need to brace ourselves for the same.</p>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Is WebAssembly future of Web Development</title><link>https://sachinsu.github.io/posts/webassembly/</link><pubDate>Tue, 02 Jun 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/webassembly/</guid><description>Over the last many years, de-facto language of the Web (specifically front-end) has been Javascript (and variants like Typescript, ECMAScript versions and so on). The Web development has been revolving around HTML+CSS+Javascript trio. It all started with support for Javascript in browsers, followed by addition of XMLHTTP API, Rich DOM Manipulation Support in Javascript. To induce order and apply patterns to Javascript&amp;rsquo;s usage in browsers, numerous frameworks and libraries were introduced like React and Vue among others.</description><content type="html"><![CDATA[<p>Over the last many years, de-facto language of the Web (specifically front-end) has been Javascript (and variants like Typescript, ECMAScript versions and so on). The Web development has been revolving around HTML+CSS+Javascript trio. It all started with support for Javascript in browsers, followed by addition of XMLHTTP API, Rich DOM Manipulation Support in Javascript. To induce order and apply patterns to Javascript&rsquo;s usage in browsers, numerous frameworks and libraries were introduced like <a href="https://reactjs.org">React</a> and <a href="https://vuejs.org">Vue</a> among others. To begin with, The target used to be browsers on Large Devices like Desktop &amp; Laptops. However, soon all sorts of devices were targetted with advent of Responsive and Progressive CSS+Javascript libraries eg. <a href="https://getbootstrap.com">Bootstrap</a>. Offline Support soon came in ref: <a href="https://electronjs.org">Electron</a> and <a href="https://web.dev/progressive-web-apps/">Progressive Web Applications</a>.</p>
<p>As a result, Javascript has become lingua franca of Web Development and is being used on server side development (<a href="https://nodejs.org">Nodejs</a>).</p>
<p>The reason for this whole rant on history (which you are most likely to be aware of) is that latest kid on the Block could possibly challenge Monopoly of Javascript (and its ilk) at far as browsers are concerned.</p>
<p><em><strong>Enter WebAssembly (A.K.A. WASM)</strong></em></p>
<h2 id="webassembly">WebAssembly</h2>
<p>As per <a href="https://en.wikipedia.org/wiki/WebAssembly">Wikipedia</a>,</p>
<blockquote>
<p>WebAssembly (often shortened to Wasm) is an open standard that defines a portable binary-code format for executable programs, and a corresponding textual assembly language, as well as interfaces for facilitating interactions between such programs and their host environment.</p>
</blockquote>
<blockquote>
<p>WebAssembly or wasm is a low-level bytecode format for in-browser client-side scripting, evolved from JavaScript. It is <a href="https://evilmartians.com/chronicles/hands-on-webassembly-try-the-basics">intermidiate representation</a>(IR) where IR is transformed into machine instructions for the client architecture by browser.</p>
</blockquote>
<p>WebAssembly executables are precompiled, hence it is possible to use a variety of programming languages to make them. This essentially means that one can use same language for Server Side as well as Client side (i.e. in browser) development like (<a href="https://docs.microsoft.com/en-us/dotnet/csharp/">C#</a> or <a href="https://golang.org/">Golang</a>).</p>
<p>WebAssembly was announced in 2015 and has since being supported by prominent browser(s) like Chrome and Firefox.</p>
<p>Along side browsers, many Vendors and open source communities/contributors have released libraries to make development of WebAssembly easy.  We will look at how a WebAssembly can be developed in C# and Golang.</p>
<p><em>Note: <a href="https://github.com/appcypher/awesome-wasm-langs">All major languages</a> now support WebAssembly.</em></p>
<h3 id="c">C#</h3>
<p>During Microsoft Build 2020 <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> event, Steve Sanderson had very good session on building WebAssembly using Blazor framework in .NET. Highly recommended to watch it.</p>
<p>Blazor scaffolding provided with .NET core allows,</p>
<ul>
<li>
<p>Blazor Server App - A Template that runs server-side inside an ASP.NET Core app and handles user interactions over a SignalR connection.</p>
</li>
<li>
<p>Blazor WebAssembly App - A Template for creating a Blazor app that runs on WebAssembly.</p>
</li>
</ul>
<p>Choosing Blazor WebAssembly App project type generates a project that has sample WebAssembly running. Overall, it makes development easy for any .NET developer easy since, it uses<a href="https://docs.microsoft.com/en-us/aspnet/core/mvc/views/razor?view=aspnetcore-3.1">Razor</a> syntax to add C# code along with HTML. During Build, it generates assembly for C# Code. When Accessed via browser, it downloads .NET runtime for WebAssembly (~ 621 KB) and the project assembly itself apart from static content (i.e. HTML files, images etc). The default scafolding includes Bootstrap CSS and prepares the UI to be responsive.</p>
<p>The repository referenced by Steve during presentation is available <a href="https://aka.ms/blazor-carchecker">here</a>.</p>
<h3 id="golang">Golang</h3>
<p><a href="https://golang.org">Go</a> has got clean, fast tooling. it produces static binaries and has superb concurrency primitives.</p>
<p><a href="https://www.vugu.org/">Vugu</a> is an open source library that allows building a Web front-end in Go using WebAssembly. Generally static binaries are bulky and Vugu has addressed it using <a href="https://tinygo.org">TinyGo</a> compiler. Vugu is still work in progress but does work great in its current form.  Check out their <a href="https://www.vugu.org/doc/start">getting started</a> page.</p>
<p>Interesting take on Journey of JavaScript and what lies ahead for it, read it <a href="https://www.swyx.io/writing/js-third-age/">here</a>.</p>
<h2 id="summary">Summary</h2>
<p>In nutshell, Concept of WebAssembly provides compelling way to have full stack development in a language of your choice. It remains to be seen how and whether it provides viable alternative to current Javascript driven ecosystem.</p>
<h3 id="useful-references">Useful References,</h3>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><a href="https://channel9.msdn.com/Events/Build/2020/BOD104">Modern Web UI with Blazor</a> <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>
]]></content></item><item><title>Using Github Actions for Automated Testing and Deployment</title><link>https://sachinsu.github.io/posts/usinggithubactions/</link><pubDate>Thu, 28 May 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/usinggithubactions/</guid><description>Background The source code of tracfee.com is hosted on Github Private.
At a High level, Tracfee&amp;rsquo;s Architecture involves,
Single Page Application using VueJS, deployed on Netlify API in Go, deployed on Oracle Cloud So far, API testing has been automated and we were looking at ways to automate deployment of both UI and API. Steps required to deploy API are less since we are using Docker to run it on VM.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>The source code of <a href="https://tracfee.com">tracfee.com</a> is hosted on Github Private.</p>
<p>At a High level, Tracfee&rsquo;s Architecture involves,</p>
<ul>
<li>Single Page Application using VueJS, deployed on <a href="https://netlify.com">Netlify</a></li>
<li>API in <a href="https://golang.org">Go</a>, deployed on <a href="https://www.oracle.com/in/cloud/">Oracle Cloud</a></li>
</ul>
<p>So far, API testing has been automated and we were looking at ways to automate deployment of both UI and API. Steps required to deploy API are less since we are using Docker to run it on VM. However, in case of Netlify, it is required to build and then upload the output folder on Netlify.</p>
<p>Accordingly, it was decided to explore Github actions to automate deployment.</p>
<h2 id="using-github-actions">Using Github actions</h2>
<p>As per <a href="https://github.com/features/actions">Github</a>,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">GitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want.
</code></pre></div><p>GitHub actions work by provisioning Virtual machine to run an Event based workflow. It provides option to provision Linux/MacOS/Windows based Virtual machines. Steps in Workflow will have to be configured in <a href="https://yaml.org">YAML</a> file. Trigger for Workflow can be (but not limited to) wide variety of events like on Push or commit on branch and so on.</p>
<p>Post trigger, set of action(s) can be configured like,</p>
<ul>
<li>Checkout the branch</li>
<li>Setup environment (Install <a href="https://github.com/actions/setup-node">Node.JS</a>)</li>
<li>Perform build</li>
<li>Deployment</li>
</ul>
<p>Github has Marketplace which has many pre-built actions available. My requirement was to,</p>
<ul>
<li>Provision Linux (i.e. ubuntu-latest) Virtual Machine</li>
<li>Checkout the code (using <a href="https://github.com/actions/checkout">actions/checkout@v2</a>)</li>
<li>Setup Node.js (using <a href="https://github.com/actions/setup-node">actions/setup-node</a>)</li>
<li>perform Build and test using <a href="https://www.npmjs.com/">NPM</a></li>
<li>Deploy to Netlify using <a href="https://github.com/netlify/actions">netlify/actions/cli@master</a></li>
<li>Any secrets required as part of Workflow can be maintained using <a href="https://help.github.com/en/actions/configuring-and-managing-workflows/creating-and-storing-encrypted-secrets">Github secrets</a></li>
</ul>
<p>Above workflow needs to be maintained in <code>.github\workflows</code> folder in the repository.</p>
<p>build.yml for <a href="https://tracfee.com">tracfee.com</a> looks like,</p>

<iframe src="//carbon.now.sh/embed/f98fbca1049e15f0360c140610fa4cc4"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>

<p>Refer Gist <a href="https://gist.github.com/sachinsu/f98fbca1049e15f0360c140610fa4cc4">here</a>.</p>
<h2 id="testing-the-build-workflow">Testing the Build Workflow</h2>
<p>After configuring the workflow steps, next question is to check whether it is possible to test it locally? Luckily, there is tool available to do this. Enter <a href="https://github.com/nektos/act">Act</a> , which is a tool to  <code>Run your GitHub Actions locally </code>.  Local testing is useful for Faster feedback. In Nutshell, Act uses local docker setup to provision container and then runs workflow steps in it. Give it a try !!</p>
<p>As a next step, Plan is to automate deployment of API on Oracle Cloud using OCI CLI interface.</p>
<h3 id="useful-references">Useful References,</h3>
<ul>
<li><a href="https://medium.com/@MarekPukaj/build-with-github-actions-host-on-netlify-ebf5fa505616">Build with GitHub Actions, host on Netlify</a></li>
<li><a href="https://blogs.oracle.com/developers/adventures-in-cicd-4-deploying-a-microservice-to-the-oracle-cloud-with-github-actions-oci-cli-edition">Adventures in CI/CD [#4]: Deploying A Microservice To The Oracle Cloud With GitHub Actions [OCI CLI Edition]</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Windows Service with Cancelable Task</title><link>https://sachinsu.github.io/posts/windowsservicecancellabletask/</link><pubDate>Tue, 05 May 2020 12:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/windowsservicecancellabletask/</guid><description>Background Recently, we had requirement wherein a process should,
Periodically (Poll) or Asynchronously (Pub-sub) listen on incoming requests/messages. The whole process is expected to be long running. Should also implement clean disposal of in-flight requests and subsequent cleanup using something similar to Cancelble Context in Go The first of the objective is somewhat dependent on mechanism (Pub/sub, Listener), protocol (TCP, HTTP etc.). For the second one, .NET framework (and .</description><content type="html"><![CDATA[<h3 id="background">Background</h3>
<p>Recently, we had requirement wherein a process should,</p>
<ul>
<li>Periodically (Poll) or Asynchronously (Pub-sub) listen on incoming requests/messages. The whole process is expected to be long running.</li>
<li>Should also implement clean disposal of in-flight requests and subsequent cleanup using something similar to Cancelble <a href="https://golang.org/pkg/context/">Context</a> in Go</li>
</ul>
<p>The first of the objective is somewhat dependent on mechanism (Pub/sub, Listener), protocol (TCP, HTTP etc.). For the second one, .NET framework (and .NET Core) offers <a href="https://docs.microsoft.com/en-us/dotnet/api/system.threading.cancellationtoken?view=netcore-3.1">CancellationToken</a>. It is maint for co-operative cancellation between threads and Task Objects. So Armed with this, is it possible to come up with a template that allows cancellation of long running task while also being deployed as Windows Service (or using systemd in Linux) ?</p>
<p>Lets get Started,</p>
<h3 id="approach">Approach</h3>
<p>We can use below to construct service,</p>
<ul>
<li><a href="http://topshelf-project.com/">Topshelf</a> - Allows Hosting  services in-process as console apps or Windows services.</li>
<li><a href="htps://nlog-project.org">NLog</a> - For Logging</li>
</ul>
<p>Accordingly, we will have below Components,</p>
<ul>
<li>Listener.cs - It wraps the long running process in a C# Task. It exposes Start and Stop functions which are essentially event handlers awaiting for Signal from the service.</li>
</ul>

<iframe src="//carbon.now.sh/embed/b2869d4f44fe14f439c6f0e60ea2b5d5?filename=Queuelistener.cs"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>

<p>Refer Gist <a href="https://gist.github.com/sachinsu/b2869d4f44fe14f439c6f0e60ea2b5d5#file-queuelistener-cs">here</a></p>
<ul>
<li>Program.cs - It configures the startup parameters for the service and initializes it. Using Topshelf, one can easily debug it as Console Application before deploying it as Service.</li>
</ul>

<iframe src="//carbon.now.sh/embed/b2869d4f44fe14f439c6f0e60ea2b5d5?filename=Program.cs"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>

<p>Refer Gist <a href="https://gist.github.com/sachinsu/b2869d4f44fe14f439c6f0e60ea2b5d5#file-program-cs">here</a></p>
<p>Above Code was targetted at .NET Framework but the same can potentially be used on .NET Core thus targetting both Windows and Linux.</p>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Optimizing .NET Code using Benchmarks</title><link>https://sachinsu.github.io/posts/usingbenchmarkdotnet/</link><pubDate>Tue, 05 May 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/usingbenchmarkdotnet/</guid><description>Background Oftentimes, we come across situation where code does not perform as per expectation. What is typically approch to address it,
Performance Testing - Visual Studio Load Tests or Third party tools like Locust, Vegeta, Gatling etc. Visual Studio Diagnostics Tools Or Use tools like Perfview/dotTrace/dotMemory to diagnose bottlenecks What if it is possible to Benchmark code for,
Set of varying parameter(s) Different runtimes (.NET Framework versions, .</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>Oftentimes, we come across situation where code does not perform as per expectation. What is typically approch to address it,</p>
<ul>
<li>Performance Testing - Visual Studio Load Tests or Third party tools like  <a href="https://locust.io/">Locust</a>, <a href="https://github.com/tsenart/vegeta">Vegeta</a>, <a href="https://gatling.io/">Gatling</a> etc.</li>
<li>Visual Studio Diagnostics Tools Or</li>
<li>Use tools like Perfview/dotTrace/dotMemory to diagnose bottlenecks</li>
</ul>
<p>What if it is possible to Benchmark code for,</p>
<ul>
<li>Set of varying parameter(s)</li>
<li>Different runtimes (.NET Framework versions, .NET core, Mono etc.) with option to Benchmark it</li>
<li>Observe Memory Allocations for diagnostics</li>
<li>Get Detailed report on execution timeline</li>
<li>Have it as part of test suite so that it can be easily executed with every iteration involving optimized code to get immediate feedback</li>
</ul>
<p>Enter <a href="https://benchmarkdotnet.org/">BenchmarkDotNet</a>, a Powerful .NET library for benchmarking. It is used by <a href="https://github.com/dotnet/performance">DotNET</a> Team, Roslyn, ASP.NET Core and many other projects.</p>
<p>Though <a href="https://benchmarkdotnet.org/">Benchmarkdotnet.org</a> has nice documentation with detailed examples, Below we will look at how to benchmark a code which is aimed at dumping in-memory list of objects to a delimited file. In real-world scenario, the list of objects could be retrieved from external data store.</p>
<p>So Lets Start.</p>
<h2 id="approach">Approach</h2>
<p>We will have below before we proceed with using BenchmarkDotNet</p>
<ul>
<li>Dummy class that represents Data Structure to be dumped to a file,</li>
</ul>
<p><code>
<iframe src="//carbon.now.sh/embed/b2d914a563b49d4dd50a4143166f27ec"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>
</code></p>
<p>Refer Gist <a href="https://gist.github.com/sachinsu/b2d914a563b49d4dd50a4143166f27ec">here</a></p>
<ul>
<li>
<p>Class <code>CardWriter.cs</code> that generates file using,</p>
<ul>
<li>Using <a href="https://docs.microsoft.com/en-us/dotnet/api/system.io.streamwriter?view=netcore-3.1">StreamWriter</a> with Buffer</li>
<li>Using Stringbuilder and StreamWriter</li>
<li>Using Open source <a href="https://joshclose.github.io/CsvHelper/">CSVHelper</a> library</li>
</ul>
</li>
</ul>
<p><code>
<iframe src="//carbon.now.sh/embed/b2d914a563b49d4dd50a4143166f27ec?filename=CardWriter.cs"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>
</code></p>
<p>Refer Gist <a href="https://gist.github.com/sachinsu/b2d914a563b49d4dd50a4143166f27ec#file-cardwriter-cs">here</a></p>
<ul>
<li>Now, let us write code to benchmark above functions with Memory Diagnostics,</li>
</ul>
<p><code>
<iframe src="//carbon.now.sh/embed/b2d914a563b49d4dd50a4143166f27ec?filename=Program.cs"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>
</code></p>
<p>Refer Gist <a href="https://gist.github.com/sachinsu/b2d914a563b49d4dd50a4143166f27ec#file-program-cs">here</a></p>
<p>Above code,</p>
<ul>
<li>Class <code>FileGeneratorBenchmark</code> - This class uses BenchmarkDotNET attributes to decorate set of functions which in turn call functions from <code>CardWriter.cs</code> class.</li>
<li>Class <code>Program</code> - General purpose class with static <code>main</code> function that invokes <code>BenchmarkRunner</code> to execute benchmarks.</li>
</ul>
<p>It is required to run these benchmarks in <code>Release</code> mode or else BenchmarkDotNet will alert about the same. After running the benchmark, It will generate detailed report like below,</p>
<p><img src="/images/capture.png" alt="Benchmark Result"></p>
<p>Report shows Memory Allocation as well as Execution time lines across  Platform (.NET Framework Vesions) and parameters.</p>
<p>References:</p>
<ul>
<li><a href="https://benchmarkdotnet.org">BenchmarkDotNet</a></li>
<li><a href="https://www.stevejgordon.co.uk/introduction-to-benchmarking-csharp-code-with-benchmark-dot-net">Introduction to Benchmarking C# Code with Benchmark .NET</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>ASP.NET Core - Mind the SameSite HTTP Cookie settings</title><link>https://sachinsu.github.io/posts/samesitecookies/</link><pubDate>Thu, 09 Apr 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/samesitecookies/</guid><description>Background A Web Application, developed in ASP.NET Core (Runtime Version 3.1.100) using Razor Pages and Web API, is expected to be launched from within third-party Web Application in iframe, with complete HTML being rendered.
During the Development, a mock HTML Page was developed to simulate launching of ASP.NET core based Web Application in iframe. Note that this page as well as Application was hosted on same IIS Server and it worked fine.</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>A Web Application, developed in ASP.NET Core (Runtime Version 3.1.100) using Razor Pages and Web API, is expected to be launched from within third-party Web Application in iframe, with complete HTML being rendered.</p>
<p>During the Development, a mock HTML Page was developed to simulate launching of ASP.NET core based Web Application in iframe. Note that this page as well as Application was hosted on same IIS Server and it worked fine. Subsequently, Web Application was deployed on Test Server and URL was shared for integration with third party Application and then it happened Boom&hellip;. i.e. Application when launched in iframe rendered HTML but none of the post request would work (returning HTTP Error 400). Careful inspection showed that,</p>
<ul>
<li>
<p>Browser&rsquo;s <a href="https://developers.google.com/web/tools/chrome-devtools">Dev tools</a> showed HTTP 400</p>
</li>
<li>
<p>There were no entries in Application&rsquo;s Log File which indicates that Request was rejected either by IIS or ASP.NET Core&rsquo;s chain of filters i.e. even before it reaches handler.</p>
</li>
<li>
<p>IIS Log depicted that Request was rejected but had no additional details. May be some of the log settings were missing.</p>
</li>
<li>
<p>Next up is to carefully look at Request sent by browser in &lsquo;Network&rsquo; tab of Dev tools. It showed that none of the cookies required by Application (i.e. for Session, CSRF token etc.) were present.</p>
</li>
</ul>
<p>Enter <a href="https://docs.microsoft.com/en-us/aspnet/core/security/samesite?view=aspnetcore-3.1">SameSite</a></p>
<h4 id="samesite">SameSite</h4>
<p>SameSite is a standard designed to provide some protection against cross-site request forgery (CSRF) attacks. Support for Samesite was added from .NET Core 2.2 and onwards. It is expected that developer will control the value of SameSite attribute using <code>HttpCookie.SameSite</code> property.Setting the SameSite property to Strict, Lax, or None results in those values being written on the network with the cookie.</p>
<ul>
<li>Cookies without SameSite header are treated as SameSite=Lax by default.</li>
<li>SameSite=None must be used to allow cross-site cookie use.</li>
<li>Cookies that assert SameSite=None must also be marked as Secure.</li>
<li>Applications that use <code>&lt;iframe&gt;</code> may experience issues with sameSite=Lax or sameSite=Strict cookies because <code>&lt;iframe&gt;</code> is treated as cross-site scenarios.</li>
<li>The value SameSite=None is not allowed by the 2016 standard and causes some implementations to treat such cookies as SameSite=Strict.</li>
</ul>
<p>The SameSite=Lax setting works for most application cookies.</p>
<p>Accordingly, below settings were made in startup.cs of the ASP.NET Core Application.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">services.AddSession(options =&gt;
            {
                options.IdleTimeout = TimeSpan.FromMinutes(30);
                options.Cookie.HttpOnly = true;
                // Samesite Settings.
                options.Cookie.SameSite = SameSiteMode.Lax;
                options.Cookie.IsEssential = true;
            });

services.AddAntiforgery(options =&gt;
{
    options.Cookie.SameSite = SameSiteMode.Lax;
});
</code></pre></div><h5 id="references">References</h5>
<ul>
<li><a href="https://techcommunity.microsoft.com/t5/iis-support-blog/samesite-cookie-updates-in-asp-net-or-how-the-net-framework-from/ba-p/1156246">SameSite cookie updates in ASP.net, or how the .Net Framework from December changed my cookie usage. </a></li>
<li><a href="https://techcommunity.microsoft.com/t5/iis-support-blog/changes-in-samesite-cookie-in-asp-net-core-and-how-it-impacts/ba-p/1150771">Changes in SameSite Cookie in ASP.NET/Core and How it Impacts the Browser (Specifically Chrome) </a></li>
<li><a href="https://http203.libsyn.com/fish-scripts-special">HTTP 203 Podcast covering CORS,CORB, Samesite</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Using Channels for High performance Producer consumer implementation</title><link>https://sachinsu.github.io/posts/channelsforproducerconsumer/</link><pubDate>Wed, 12 Feb 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/channelsforproducerconsumer/</guid><description>Background Recently, i got involved in assignment where in an application was facing issues with throughput. Expectation is to support more than 500 transactions per second while load testing results were indicating system was experiencing high latency beyond 100+ transactions per second.
This application is developed in .NET Framework + .NET Core and primarily uses Relational Database for persistence and has point to point integration (mainly over HTTP) with internal &amp;amp; external application(s).</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>Recently, i got involved in assignment where in  an application was facing issues with throughput. Expectation is to support more than 500 transactions per second while load testing results were indicating system was experiencing high latency beyond 100+ transactions per second.</p>
<p>This application is developed in .NET Framework + .NET Core and primarily uses Relational Database for persistence and has point to point integration (mainly over HTTP) with internal &amp; external application(s).</p>
<h2 id="approach">Approach</h2>
<p>The high level approach decided to perform diagnostics and subsequent corrective action(s) were,</p>
<ul>
<li>Benchmark code that involves Database and take corrective action</li>
<li>Identify tasks in hot code path that could potentially be decoupled or done in fire-n-forget mode.</li>
</ul>
<p>For point 2 from above, some of the tasks identified were,</p>
<ul>
<li>Sending Email/SMS on myriad of events</li>
<li>Integration with External Applications over HTTP</li>
</ul>
<p>Next task was to arrive at approach on how to perform them effectively outside of hot code path without incurring need of any additional resources (hardware or software)as far as possible. Accordingly, we had two options,</p>
<ul>
<li><em>Polling</em> - Periodically polling database to check for occurance of event and then performing the action.</li>
<li><em>Event Driven</em> - Using Event notification feature of database (e.g. <a href="https://www.postgresql.org/docs/current/sql-notify.html">Listen/Notify</a> in PostgreSQL or <a href="https://docs.oracle.com/cd/B28359_01/appdev.111/b28424/adfns_cqn.htm">Change Notification</a>/<a href="https://www.oracle.com/database/technologies/advanced-queuing.html">Advanced Queuing</a> in Oracle).</li>
</ul>
<p>We decided to go with <em>Event driven</em> as,</p>
<ul>
<li>Cleaner approach that doesn&rsquo;t require perodically checking for events thus consuming a database connection and more code.</li>
<li>We may have to have more than one such daemons to cater to different events in application.</li>
</ul>
<p>Post finalizing on event driven approach for gathering events, next task was to determine how to effectively send email/sms or any other HTTP requests considering that rate of arrival of events will not be matching rate of processing them. Also these</p>
<p>So what are the options available in .NET Ecosystem, Below are the ones i am aware of,</p>
<ul>
<li><a href="https://devblogs.microsoft.com/dotnet/an-introduction-to-system-threading-channels/">Channels</a> - High performance implementation of In-memory producer/consumer pattern.</li>
<li><a href="https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/dataflow-task-parallel-library">TPL Dataflow</a> - Super set of Channels Library. Aimed at use cases where blocks of logic are to be linked together to same or different consumers and so on. Also all these features come with additional overheads.</li>
</ul>
<p>For the task at hand, functionality offered by Channels is sufficient to implement in-memory producer consumer pattern.</p>
<p>So we wrapped above event processing in a Windows service implemented as <a href="https://docs.microsoft.com/en-us/aspnet/core/fundamentals/host/hosted-services?view=aspnetcore-3.1&amp;tabs=visual-studio">.NET Core Worker
Service</a></p>
<p>Generic Implementation is as follows,</p>
<ul>
<li>
<p>Event Generator - In practice, this class will be responsible for wiring up to receive events from database</p>
</li>
<li>
<p>Event Consumer which uses channels to process events in parallel</p>
</li>
</ul>

<iframe src="//carbon.now.sh/embed/6fcbc36e6e5cc58c7b5ba9007e276afc"
    style="transform:scale(0.9); width:1024px; height:473px; border:0; overflow:hidden;"
    sandbox="allow-scripts allow-same-origin">
</iframe>

<p>Refer Gist <a href="https://gist.github.com/sachinsu/6fcbc36e6e5cc58c7b5ba9007e276afc">here</a></p>
<h2 id="summary">Summary</h2>
<p>Other languages (notably <a href="https://tour.golang.org/concurrency/2">Channels in Go</a>) have been providing out of the box implementation for in-memory producer with concurrent, parallel consumers. With Channels, .NET Ecosystem finally has construct that can be effectively put to use for high performance, concurrent use cases.</p>
<h3 id="useful-references">Useful References,</h3>
<ul>
<li><a href="https://docs.microsoft.com/en-us/dotnet/csharp/event-pattern#defining-and-raising-field-like-events">Event Pattern in C#</a></li>
<li><a href="https://gist.github.com/AlgorithmsAreCool/b0960ce8a3400305e43fe8ffdf89b32c">Gist on using Channels</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item><item><title>Using .NET standard Assembly in .NET core and .NET Framework</title><link>https://sachinsu.github.io/posts/dotnetstandard/</link><pubDate>Fri, 07 Feb 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/dotnetstandard/</guid><description>Background One of the key project(s) at my current organization is developed on .NET 4.6.1. It is developed as Modular Monolith. As part of it&amp;rsquo;s functionality, it supports different channels like Mobiles, Terminals and Web. For the Web channel, there was need to develop a Web application with,
High availability Lightweight, High throughput (Need to support few thousand(s) active users) Accordingly, we have been exploring developing this Web Application in .</description><content type="html"><![CDATA[<h2 id="background">Background</h2>
<p>One of the key project(s) at my current organization is developed on .NET 4.6.1. It is developed as <a href="https://www.youtube.com/watch?v=5OjqD-ow8GE">Modular Monolith</a>. As part of it&rsquo;s functionality, it supports different channels like Mobiles, Terminals and Web. For the <em>Web</em> channel, there was need to develop a Web application with,</p>
<ul>
<li>High availability</li>
<li>Lightweight, High throughput (Need to support few thousand(s) active users)</li>
</ul>
<p>Accordingly, we have been exploring developing this Web Application in .NET core 3.1. However, it also means that we will have to use class libraries, targeted at .NET framework 4.6.1, in .NET core and vice-versa. How can this be done?</p>
<p><strong>.NET Standard to the rescue !!</strong></p>
<p><a href="https://docs.microsoft.com/en-us/dotnet/standard/net-standard">.Net Standard</a> is a standard that enabled development of portable libraries usable across .NET versions.</p>
<p>Below is approach adopted to create usable libraries across .NET framework &amp; .NET Core.</p>
<ul>
<li>
<p>.NET &amp; IDE versions used are,</p>
<ul>
<li>.Net Framework 4.6.1</li>
<li>.Net core 3.1</li>
<li>Visual Studio 2015 - for .NET Framework 4.6.1 development</li>
<li>Visual Studio Code - For .NET core development</li>
</ul>
</li>
<li>
<p>Step 1 -</p>
<ul>
<li>Create a library that targets .NET Standard.
<ul>
<li>
<p>Refer to Table on <a href="https://docs.microsoft.com/en-us/dotnet/standard/net-standard#net-implementation-support">Implementation Support</a> to decide on version that can be targetted at. In my case, it was 2.0 (Remember that higher the version, more APIs will be available to use). Do check <a href="https://docs.microsoft.com/en-us/dotnet/api/">.NET API browser</a>, which lists API available with each version.</p>
</li>
<li>
<p>Using .NET core, use below command,
<code>dotnet new classlib &lt;name&gt;</code></p>
<p>Note that, by default csproj file generated targets .NET Standard, but do confirm by checking in <code>&lt;name&gt;.csproj</code> file, It should have entry like,</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">&lt;PropertyGroup&gt;
    &lt;TargetFramework&gt;netstandard2.0&lt;/TargetFramework&gt;
&lt;/PropertyGroup&gt;
</code></pre></div><p>Change the version of .NET Standard if required.</p>
</li>
<li>
<p>Add necessary code to the library and build it using,
<code>dotnet build</code></p>
</li>
<li>
<p>Create a Nuget Package using,
<code>dotnet pack</code>
This will generate <code>&lt;name&gt;1.0.0.nupkg</code> package in bin\debug folder (assuming that you are using Debug mode)</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Step 2 -</p>
<ul>
<li>
<p>Lets consume this library from console Application, using .NET Framework 4.6.1, in Visual Studio 2015.</p>
<ul>
<li>
<p>Create New Console Application and ensure that it is targeted at .NET Framework 4.6.1 or Higher.</p>
</li>
<li>
<p>Before consuming .NET standard library, few steps are needed since VS 2015 only has legacy support for consuming .NET core artifacts also it does not have latest version of Nuget, so lets do below,</p>
<ul>
<li>Install NuGet 3.6.0 or higher for VS 2015 from <a href="https://www.nuget.org/downloads">NuGet&rsquo;s download site</a></li>
<li>Install the &ldquo;.NET Standard Support for Visual Studio 2015&rdquo; from <a href="https://www.microsoft.com/net/download/core">here</a></li>
<li>Open the csproj file in Text Editor and add  <code>&lt;ImplicitlyExpandDesignTimeFacades&gt;</code> tag as shown in below example,</li>
</ul>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-fallback" data-lang="fallback">&lt;?xml version=&#34;1.0&#34; encoding=&#34;utf-8&#34;?&gt;
&lt;Project ToolsVersion=&#34;12.0&#34; DefaultTargets=&#34;Build&#34; xmlns=&#34;http://schemas.microsoft.com/developer/msbuild/2003&#34;&gt;
&lt;PropertyGroup&gt;
    &lt;Configuration Condition=&#34; &#39;$(Configuration)&#39; == &#39;&#39; &#34;&gt;Debug&lt;/Configuration&gt;
    &lt;Platform Condition=&#34; &#39;$(Platform)&#39; == &#39;&#39; &#34;&gt;AnyCPU&lt;/Platform&gt;
    &lt;ProjectGuid&gt;{75678902-8224-4222-BB33-756784B2FA29}&lt;/ProjectGuid&gt;
    &lt;OutputType&gt;Library&lt;/OutputType&gt;
    &lt;RootNamespace&gt;FooBar&lt;/RootNamespace&gt;
    &lt;AssemblyName&gt;FooBar&lt;/AssemblyName&gt;
    &lt;TargetFrameworkVersion&gt;v4.6.1&lt;/TargetFrameworkVersion&gt;
    ...
    &lt;ImplicitlyExpandDesignTimeFacades&gt;false&lt;/ImplicitlyExpandDesignTimeFacades&gt;
&lt;/PropertyGroup&gt;
</code></pre></div><p>Post update to file, VS 2015 will prompt to reload the project.</p>
<p>Now we are set to consume .NET standard library, authored in .NET Core, in this project.</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Step 3 -</p>
<ul>
<li>Within VS 2015, Goto Nuget Console and install the package created earlier. This <a href="https://docs.microsoft.com/en-us/nuget/consume-packages/install-use-packages-visual-studio">link</a> has steps to consume local nuget package(s).</li>
</ul>
</li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>
]]></content></item></channel></rss>